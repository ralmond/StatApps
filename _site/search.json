[
  {
    "objectID": "Geyser.html",
    "href": "Geyser.html",
    "title": "Histogram Bins",
    "section": "",
    "text": "The data show the duration (in minutes) of eruptions of the Geyser ‘Old Faithful’ in Yellowstone National Park, Wyoming. Härdle (1991)."
  },
  {
    "objectID": "Geyser.html#bin-size-and-bandwidth.",
    "href": "Geyser.html#bin-size-and-bandwidth.",
    "title": "Histogram Bins",
    "section": "Bin size and bandwidth.",
    "text": "Bin size and bandwidth.\nThe number of histograms in a describe how much detail you get. Try adjusting thing number to see the effect. Too few bins might cause the viewer to miss out on important details. Too many bins might cause the viewer to see details that are just an artifact of the sample (which might be different if the data were taken from a different window of time).\nThe smooth line is a kernel smoother (a technique which available in R, but not SPSS). The bandwidth of the smoother is like the bin size of the histogram. Large bandwidths provide less detail and smaller provide more.\nWith both bin sizes and bandwidths: * The analyst sometimes need to try a few values to find the best one for your purposes. * The defaults in the stat packages are usually a good starting point. Try more bins (or smaller bandwidth) and fewer bins (less bandwidth) than the automatically chosen starting point.\n\n\n\nAttaching package: 'shiny'\n\n\nThe following object is masked from 'package:shinyjs':\n\n    runExample\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:\n\n\n\n\n\n\n\n\n\n\nHärdle, W. (1991). Smoothing Techniques with Implementation in S. New York: Springer."
  },
  {
    "objectID": "Geyser.html#credits",
    "href": "Geyser.html#credits",
    "title": "Histogram Bins",
    "section": "Credits",
    "text": "Credits\nI borrowed this document from one of the Shiny sample documents. Original instructions below. (Russell Almond).\nThis R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "CovidVaccines.html",
    "href": "CovidVaccines.html",
    "title": "Covid-19 Vaccines",
    "section": "",
    "text": "Good News! In November, both Pfizer and Moderna announce Phase 3 Vaccine Trials with promising results.\nWith over 30,000 participants in each study they reported the following data.\ncovidVaccines &lt;- tibble(\n  Treatment=c(\"Placebo\",\"Vaccine\"),\n  Pfizer=c(90,5),ModernaAll=c(95,5),\n  ModernaSevere=c(11,0),\n  N=c(15000,15000))\ncovidVaccines.N &lt;- 15000\nkable(covidVaccines)\n\n\n\n\nTreatment\nPfizer\nModernaAll\nModernaSevere\nN\n\n\n\n\nPlacebo\n90\n95\n11\n15000\n\n\nVaccine\n5\n5\n0\n15000"
  },
  {
    "objectID": "CovidVaccines.html#measures-of-effectiveness",
    "href": "CovidVaccines.html#measures-of-effectiveness",
    "title": "Covid-19 Vaccines",
    "section": "Measures of Effectiveness",
    "text": "Measures of Effectiveness\nWe start with a cross-tab\n\n\n\nTreatment\nSick\nHealthy\nTotal\n\n\n\n\nPlacebo\nSP\nHP\nNP\n\n\nVaccine\nSV\nHV\nNV\n\n\nTotal\nNS\nNH\nN\n\n\n\nOdds of getting sick\nPlacebo: $ SP/HP $ Vaccine: $ SV/HV $\n\nCross Product (Odds) Ratio\n\\[ OR = \\frac{SP/HP}{SV/HV} \\] How much does your odds of getting sick increase if you get the placebo instead of the vaccine.\n\n\nRisk Ratio\n\\[ RR = \\frac{SP/NP}{SV/NV} \\] How much does your probiliby of getting sick increase if you get the placebo instead of the vaccine.\n\n\nVaccine Effectiveness\n\\[ VE = 100 (1 - \\frac{1}{RR}) \\]\n\n\nChi-square test\nNull hypothesis is that getting the disease is independent of the vaccine. In other words, \\(OR=RR=1\\).\n\\[ SV/NV = SP/NP \\]\nLarge chi-squared value incidates that cross product rate is not 1 (but doesn’t tell if placebo or vaccine is better!\n\n\nZ-score test\nAnother way to work with these data is to calculate probabilities of infection for each group and the standard errors. Then can use the \\(z\\)-test to compare.\n\\[p_V = p(S|V) = SV/NV \\qquad SE(p_V) = \\sqrt{p_V(1-p_V)/NV} \\] \\[p_P = p(S|P) = SP/NP \\qquad SE(p_P) = \\sqrt{p_P(1-p_P)/NP} \\] \\[ z = \\frac{p_P-p_V}{\\sqrt{SE(p_V)^2 + SE(p_P)^2}}\\]"
  },
  {
    "objectID": "CovidVaccines.html#pfizer-vaccine",
    "href": "CovidVaccines.html#pfizer-vaccine",
    "title": "Covid-19 Vaccines",
    "section": "Pfizer Vaccine",
    "text": "Pfizer Vaccine\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.Pfizer=Pfizer/N) %&gt;%\n  mutate(s.Pfizer=sqrt(p.Pfizer*(1-p.Pfizer)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"Pfizer\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nPfizer\np.Pfizer\ns.Pfizer\n\n\n\n\nPlacebo\n90\n6e-03\n0.00063\n\n\nVaccine\n5\n3e-04\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.Pfizer,ymin=p.Pfizer-2*s.Pfizer,ymax=p.Pfizer+2*s.Pfizer)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Pfizer\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nPfizer Cross Tab\n\n\n\n\n\nPhizer X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,Pfizer,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, Pfizer, N))\nX-squared = 74.034, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\ns &lt;- pull(covidVaccines,s.Pfizer)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  8.75 p =  0 \n\n\n\n\n\nPfizer Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.Pfizer)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  18 Vaccine Effectiveness:  94.4"
  },
  {
    "objectID": "CovidVaccines.html#moderna-vaccine-all-cases",
    "href": "CovidVaccines.html#moderna-vaccine-all-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – All Cases",
    "text": "Moderna Vaccine – All Cases\nThere were around 30,000 volunteers in the Phase 3 trials; 15,000 in each arm.\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaAll=ModernaAll/N) %&gt;%\n  mutate(s.ModernaAll=sqrt(p.ModernaAll*(1-p.ModernaAll)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaAll\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaAll\np.ModernaAll\ns.ModernaAll\n\n\n\n\nPlacebo\n95\n0.0063\n0.00065\n\n\nVaccine\n5\n0.0003\n0.00015\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaAll,ymin=p.ModernaAll-2*s.ModernaAll,ymax=p.ModernaAll+2*s.ModernaAll)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna All Cases Cross Tab\n\n\n\n\n\nModerna All Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaAll,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaAll, N))\nX-squared = 78.942, df = 1, p-value &lt; 2.2e-16\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\ns &lt;- pull(covidVaccines,s.ModernaAll)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  9.03 p =  0 \n\n\n\n\n\nModerna All Cases Risk Ratio\n\n\n\np &lt;- pull(covidVaccines,p.ModernaAll)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  19 Vaccine Effectiveness:  94.7"
  },
  {
    "objectID": "CovidVaccines.html#moderna-vaccine-severe-cases",
    "href": "CovidVaccines.html#moderna-vaccine-severe-cases",
    "title": "Covid-19 Vaccines",
    "section": "Moderna Vaccine – Severe Cases",
    "text": "Moderna Vaccine – Severe Cases\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=ModernaSevere/N) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/N)) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(4,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n7e-04\n0.00022\n\n\nVaccine\n0\n0e+00\n0.00000\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\nX^2 – Moderna (All Cases)\nIn SPSS this is done by producing a cross-tab. We don’t have the number of negative cases in each arm of the study, but up to rounding error it is just the sample size, so we will use that.\n\n\n\nModerna Severe Cases Cross Tab\n\n\n\n\n\nModerna Severe Cases X2\n\n\n\nchisq.test(as.matrix(select(covidVaccines,ModernaSevere,N)))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  as.matrix(select(covidVaccines, ModernaSevere, N))\nX-squared = 9.0869, df = 1, p-value = 0.002574\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\ns &lt;- pull(covidVaccines,s.ModernaSevere)\n\nz &lt;- (p[1]-p[2])/sqrt(sum(s^2))\npz &lt;- 1-pnorm(z)\ncat(\"Z = \",round(z,2), \"p = \",round(pz,3),\"\\n\")\n\nZ =  3.32 p =  0 \n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  Inf Vaccine Effectiveness:  100 \n\n\nYikes! The estimate for the chances of getting Severe Covid-19 with the virus is 0. Divide by zero error!\nBut probability zero means impossible. That is not right!\n\n\nContinuity Correction\nFix this by adding a conditinuity correction. We add 1/2 to all of the entries in the table.\nIn particular, this makes the estimated rate for getting severe COVID-19 \\(\\frac{1}{2}/(N+1)\\).\n\ncovidVaccines %&gt;% \n  mutate(p.ModernaSevere=(ModernaSevere+.5)/(N+1)) %&gt;%\n  mutate(s.ModernaSevere=sqrt(p.ModernaSevere*(1-p.ModernaSevere)/(N+1))) -&gt;\n  covidVaccines\n\nselect(covidVaccines,Treatment,contains(\"ModernaSevere\")) %&gt;% kable(digits=c(5,5))\n\n\n\n\nTreatment\nModernaSevere\np.ModernaSevere\ns.ModernaSevere\n\n\n\n\nPlacebo\n11\n0.00077\n0.00023\n\n\nVaccine\n0\n0.00003\n0.00005\n\n\n\n\n\n\nggplot(covidVaccines,aes(x=Treatment,y=p.ModernaSevere,ymin=p.ModernaSevere-2*s.ModernaSevere,ymax=p.ModernaSevere+2*s.ModernaSevere)) + geom_pointrange()\n\n\n\n\n\n\n\n\n\np &lt;- pull(covidVaccines,p.ModernaSevere)\nRR &lt;- p[1]/p[2]\nVE &lt;- 100*(1-1/RR)\ncat(\"Risk Ratio: \",round(RR,2),\n    \"Vaccine Effectiveness: \",round(VE,1),\"\\n\")\n\nRisk Ratio:  23 Vaccine Effectiveness:  95.7"
  },
  {
    "objectID": "CovidVaccines.html#references",
    "href": "CovidVaccines.html#references",
    "title": "Covid-19 Vaccines",
    "section": "References:",
    "text": "References:\n\nStatNews article on Pfizer vaccine: https://www.statnews.com/2020/11/09/covid-19-vaccine-from-pfizer-and-biontech-is-strongly-effective-early-data-from-large-trial-indicate/\nOfficial Protocol document from Pfizer: https://www.pfizer.com/science/coronavirus\nPfizer Press Release: https://www.pfizer.com/news/press-release/press-release-detail/pfizer-and-biontech-announce-vaccine-candidate-against\nModerna Press Release: https://investors.modernatx.com/news-releases/news-release-details/modernas-covid-19-vaccine-candidate-meets-its-primary-efficacy\nEntries from Andrew Gelman’s Blog: https://statmodeling.stat.columbia.edu/2020/11/16/estimating-efficacy-of-the-vaccine-from-95-true-infections/\n\nhttps://statmodeling.stat.columbia.edu/2020/11/11/the-pfizer-biontech-vaccine-may-be-a-lot-more-effective-than-you-think/\n\nHow to use SPSS to obtain Odd Ratio and Relative Risk http://brahms.emu.edu.tr/icetin/spss8-RR-OR.pdf"
  },
  {
    "objectID": "ConfidenceInterval.html",
    "href": "ConfidenceInterval.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "ConfidenceInterval.html#confidence-intervals",
    "href": "ConfidenceInterval.html#confidence-intervals",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The simple heuristic for the confidence interval is that if we take a sample and calculate an estimator and a standard error for that estimator; 95% of the time the estimand will be within 2 standard errors of the estimate. This heuristic works best for sample means, because by the central limit theorem the distribution of the sample mean will be approximately normal. It also works fairly well for other statistics, like regression slopes.\nLet the goal be to produce an interval which \\((1-\\alpha)\\)% of the time captures the estimand. If we assume that the estimate, \\(\\widetilde{f(Y)}\\), is approximately normally distributed with a mean of the estimand, \\(f(Y)\\) and a standard deviation of the standard error of the estimate, \\(\\sigma_{\\widetilde{f(Y)}}\\), (i.e., we are assuming that central limit theorem holds at least approximately for \\(\\widetilde{f(Y)}\\)), then we can produce a confidence interval using the following expression: \\[ \\widetilde{f(Y)} \\pm z_{1-\\alpha/2}\\ \\sigma_{\\widetilde{f(Y)}}\\; .\\]\nUgh. Lets break this apart using an example. Let’s say what we are interested in is the sample mean. Then \\(\\widetilde{f(Y)} = \\bar Y\\) is just the sample mean. The quantile \\(z_{1-\\alpha/2}\\) depends on the desired accuracy. The default choice is \\(1-\\alpha=.95\\), so \\(1-\\alpha/2 = .975\\), and looking this up on the normal table \\(z_{.975}\\approx 1.96 \\approx 2\\). Finally, \\(\\sigma_{\\widetilde{f(Y)}}\\) is the standard error of the mean, so if the population standard deviation of \\(Y\\) is \\(\\sigma_Y\\) and the sample size is \\(N\\), then \\(\\sigma_{\\widetilde{f(Y)}} = \\sigma_Y/\\sqrt{N}\\). This gives us the slightly easier to look at: \\[\\bar Y \\pm 1.96 \\sigma_Y/\\sqrt{N} \\approx \\bar Y \\pm 2 \\sigma_Y/\\sqrt{N}\\ ;\\] Note that we are assuming that \\(\\sigma_Y\\) is known here. If we need to estimate it from the data, we need a slightly different formula given later."
  },
  {
    "objectID": "ConfidenceInterval.html#catching-fish-in-our-net",
    "href": "ConfidenceInterval.html#catching-fish-in-our-net",
    "title": "Confidence Intervals",
    "section": "Catching Fish in Our Net",
    "text": "Catching Fish in Our Net\nThere are two interpretations of confidence intervals (c.i.s): classical and Bayesian (although the latter are often called credibility intervals to distinguish them). As the Bayesian interpretation requires fewer assumptions, we will explore it first.\nIn the classical interpretation the c.i., the c.i. is like a net that is cast into the sea. It either will or will not catch the fish (the estimand). On average, the c.i. will catch the fish \\((1-\\alpha)\\)% of the time; this probability comes from the sampling. On a given time, we either will or will not have the fish in the net, but if we through it out many times, we will catch the fish \\((1-\\alpha)\\)% of the time."
  },
  {
    "objectID": "ConfidenceInterval.html#random-points",
    "href": "ConfidenceInterval.html#random-points",
    "title": "Confidence Intervals",
    "section": "Random Points",
    "text": "Random Points\nSelect the number of repetitions (how many times we through the net), the sample size (the size of the net) as well as the parameters of the population.\nYou may need to press the regenerate button to get the graph to have the right symbols.\n#| standalone: true\n#| viewerHeight: 1500\nlibrary(shiny)\n\nrpointnotes &lt;- \"\"\nrintnotes &lt;-\"\"\n\n\nZ &lt;- rnorm(100)\np1 &lt;- floor(abs(Z)) +1\nui &lt;- fluidPage(\ninputPanel(\n    selectInput(\"M\", label = \"Number of Repetitions:\",\n                choices = c(50, 100, 200), selected = 100),\n    selectInput(\"N\", label = \"Sample Size:\",\n                choices = c(1,5,10,25,50,100), selected = 1),\n    sliderInput(\"my\", label = \"Mean of Y:\",\n              min=0, max=100, value=50, step=1),\n    sliderInput(\"sy\", label = \"Standard Deviation of Y:\",\n              min = 0.2, max = 25, value = 10, step = 0.1),\n    actionButton(\"go\",label=\"(Re)Generate\")),\nmainPanel(\n  plotOutput(\"plot\"),\n  fluidRow(textOutput(\"sem\")),\n  fluidRow(textOutput(\"text1\")),\n  fluidRow(textOutput(\"text2\")),\n  fluidRow(textOutput(\"text3\")),\n  tags$ul(\n    tags$li(\"Approximaly 2/3 of the data points should be within 1 SE of the mean (plotted as circles)\"),\n    tags$li(\"* Approximately 95 percent of the data points should be within 1 SE of the mean (circles and triangles).\"),\n    tags$li(\"Approximately 5 percent of the data points should be 2 SEs or more away from the mean (plotted at diamonds).\")),\n  p(\"Note that changing the mean and sd of the population only changes the scales on the graph, not the structure of the problem.\"),\n  h2(\"Random Intervals\"),\n  p(\"Taking the sample mean and going plus or minus two standard errors produces a confidence interval.\"),\n  markdown(\"Actually, the two standard error rule is based on looking up the .975 (1-.05/2) point on the [normal table](NormalCalculator.Rmd).  We could put other values in there as well (50%, 75%, 90% and 99% are common choices).  This will adjust the length of the slider.\"),\n  sliderInput(\"alpha\",\"Confidence\",\n                       min=0,max=1,value=.95,step=.01),\n  plotOutput(\"plotI\"))\n               \n\nserver &lt;- function (input,output) {\n\n  dataSet &lt;- reactiveValues(Z=Z,sem=10,\n                             pch=ifelse(p1 &gt;2, 5, p1), \n                         M=100,\n                 my=50,sy=10)\n  \n  observeEvent(input$go,{\n      M &lt;- as.numeric(input$M)\n      dataSet$Z &lt;- rnorm(M)\n      sy &lt;- as.numeric(input$sy)\n      dataSet$sem &lt;- sy / sqrt(as.numeric(input$N))\n      p1 &lt;-floor(abs(dataSet$Z)) + 1\n      dataSet$pch &lt;- ifelse(p1 &gt; 2, 5, p1)\n      my &lt;- as.numeric(input$my)\n  })\n\n  output$plot &lt;- renderPlot({\n      sem &lt;- dataSet$sem\n      my &lt;- dataSet$my\n      sy &lt;- dataSet$sy\n      X &lt;- dataSet$Z * sem + my\n      M &lt;- length(X)\n      curve(\n        dnorm(x, my, sem),\n        xlim = c(my - 3.5 * sy, \n                 my + 3.5 * sy),\n        ylab = \"density\",\n        xlab = \"Sample Mean\"\n      )\n      abline(v = my)\n      abline(h = 0)\n      text(my + .25 * sem, .02, expression(mu[Y]))\n      abline(v = my - 2 * sem)\n      text(my - 2 * sem + .25 * sem, 0.0025, \"-2SE\")\n      abline(v = my - sem)\n      text(my - sem + .25 * sem, 0.005,\"-1SE\")\n      abline(v = my + sem)\n      text(my + sem + .25 * sem, 0.005, \"+1SE\")\n      abline(v = my + 2 * sem)\n      text(my + 2 * sem + .25 * sy, 0.0025, \"2SE\")\n      points(X, rep(0, M), pch = dataSet$pch)\n    })\n   output$sem &lt;- renderText({\n     sem &lt;- dataSet$sem\n      paste(\n        \"Standard Error = \",\n        round(sem, 3),\".\")})\n   output$text1 &lt;- renderText({\n     pch &lt;- dataSet$pch\n     paste(sum(pch == 1),\n        \"Estimates less than 1 SE from mean;\")\n    })\n   output$text2 &lt;- renderText({\n     pch &lt;- dataSet$pch\n     paste(sum(pch == 2),\n        \"Estimates between 1 and 2 SE from mean;\")\n    })\n   output$text3 &lt;- renderText({\n     pch &lt;- dataSet$pch\n     paste(sum(pch == 5),\n        \"Estimates more than 2 SE from mean.\")\n   })\n   output$plot1 &lt;- renderPlot({\n      sem &lt;- dataSet$sem\n      my &lt;- dataSet$my\n      X &lt;- dataSet$Z * sem + my\n      M &lt;- length(X)\n      i &lt;- 1:M\n      alpha &lt;- (1-as.numeric(input$alpha))/2\n      X.low &lt;- X +qnorm(alpha)*sem\n      X.high &lt;- X + qnorm(1-alpha)*sem\n      pch1 &lt;- ifelse(X.low &lt;= my & my &lt;= X.high,1,5)\n      plot(c(my-3.5*sem,my+3.5*sem),c(0,M+1),\n             ylab=\"Trial\",xlab=\"Sample Mean\",\n           main=paste(100*(1-2*alpha),\"% Confidence Intervals\"), type=\"n\")\n      points(X,i,pch=pch1)\n      segments(X.low,i,X.high,i,lty=pch1,\n               col=ifelse(pch1==5,1,5))\n      abline(v = my)\n    })\n\n\n\n}\nshinyApp(ui=ui,server=server)\nThe number of confidence intervals that don’t overlap the target line should be around \\(\\alpha\\) (the number in the slider) of the total number of intervals.\n\nThis graph and the random points above are based on the same data. For the 95% interval; the data points where the intervals don’t cross correspond to the data points outside of the 95% region."
  },
  {
    "objectID": "ConfidenceInterval.html#random-intervals",
    "href": "ConfidenceInterval.html#random-intervals",
    "title": "Confidence Intervals",
    "section": "Random Intervals",
    "text": "Random Intervals\nTaking the sample mean and going plus or minus two standard errors produces a confidence interval.\nActually, the two standard error rule is based on looking up the .975 (1-.05/2) point on the normal table. We could put other values in there as well (50%, 75%, 90% and 99% are common choices). This will adjust the length of the slider.\n\n\n\n\n\n\nConfidence\n\n\n\n\n\n\n\n\n\n\nThe number of confidence intervals that don’t overlap the target line should be around \\(\\alpha\\) (the number in the slider) of the total number of intervals.\n\nThis graph and the random points above are based on the same data. For the 95% interval; the data points where the intervals don’t cross correspond to the data points outside of the 95% region."
  },
  {
    "objectID": "ConfidenceInterval.html#interpreting-confidence-intervals",
    "href": "ConfidenceInterval.html#interpreting-confidence-intervals",
    "title": "Confidence Intervals",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nThe number \\(\\alpha\\), most often 95%, is known as the level of the confidence interval. The level is interpreted as a probability, but there are two schools of thought for interpreting it.\n\nClassical Approach\nThe classical statistical paradigm regards the population mean as a fixed but unknown quantity. The true value is either in or not in the interval, we don’t know which.\nRandom variability comes from the sampling, therefore the 95% comes from imagining different worlds in which we repeated the same sampling and analysis over and over again. 95% of the time, our net (the interval) will catch the fish.\nIn the classical paradigm, we can’t say that their is a 95% chance that the fish is in the net, as we can’t express the position of the fish as a probability: only the position of the net.\n\n\nBayesian Approach\nThe Bayesian paradigm makes the position of the fish a random variable. To do that, however, it needs an additional assumption: a probability distribution for the initial position of the fish.\nFor simplicity, assume that all positions of the fish are [equally likely]1. Using that assumption and Bayes’s Theorem, calculate the posterior probability of the fish (after observing the data). The interval that is constructed is called a credibility interval. There is a 95% chance that the fish is in the credibility interval (at least if the model, both the prior assumption and the normal distribution of the data is correct).\n\n\nWhich approach is better\nActually, most people want both interpretations to hold. They want a proceedure that catches the fish 95% of the time and they want the fish to be in the net 95% of the time.\nFortunately, when we use the approximation that all positions of the fish are equally likely, the two intervals are the same. The abbreviation c.i. could stand for either confidence or credibility interval.\nThe Bayesian interpretation relies on an additional assumption, but both intervals rely on assumptions about the distribution of the observed data. In particular, in this example, we are using the normal distribution to calculate the intervals. That means that the distribution must be close enough to normal that by the central limit theorem, it is reasonable to think that the mean is approximately normally distributed.\nBoth c.i.s break down if the there is a problem with the sample. If this was a convenience sample and not a random one, the normal distribution around the population mean might not be at all right. The c.i. only talks about random error and not bias."
  },
  {
    "objectID": "ConfidenceInterval.html#footnotes",
    "href": "ConfidenceInterval.html#footnotes",
    "title": "Confidence Intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe equally likely assumption is actually a bit nonsensical as we probably expect the fish to in the middle of the pond and not out past the orbit of Pluto. However, if we have enough data, the assumption will not play a big role in our estimate.↩︎"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Test",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "Untitled.html#r-markdown",
    "href": "Untitled.html#r-markdown",
    "title": "Test",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "Untitled.html#including-plots",
    "href": "Untitled.html#including-plots",
    "title": "Test",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "CorrelationOutliers.html",
    "href": "CorrelationOutliers.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n10\n25\n50\n100\n250\n500\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "CorrelationOutliers.html#the-effect-of-outliers.",
    "href": "CorrelationOutliers.html#the-effect-of-outliers.",
    "title": "Correlation Coefficient",
    "section": "The effect of outliers.",
    "text": "The effect of outliers.\nThe data set below has \\(N\\) data points. The sliders are hooked up to the first one (which is plotted in red). The rest are generated from a normal distribution with mean 0 and standard deviation 1. They are uncorrelated, but there often will be a small correlation because of sampling variability.\n\n\n\n\n\n\nX-coordinate of point 1:\n\n\n\n\n\nY-coordinate of point 1:\n\n\n\n\n\n\n\n\n\n\n\nOutliers in Y\nSet the \\(X\\) value for the red point to zero. Now move the \\(Y\\) value up and down. How sensitive is the correlation to changes in the \\(Y\\) value with \\(X=\\bar X\\)?\n\n\nLeverage Points: Outliers in X\nNow set \\(X\\) to a high value (away from the mean at 0). Again move \\(Y\\) up and down, what happens to the line? Set \\(X\\) to a low value and try again.\nValues which are outliers in the \\(X\\) variable (or in the case of multiple regression, one or more of the \\(X\\) variables) are known as leverage points or influential points.\n\n\nSample Size\nNow try changing the sample size in the dialogues at the top of the page. Note that you will need to tweak one of the sliders for the graph to redraw at the new sample size. Is the correlation more or less sensitive at low sample sizes? At high sample sizes?\nAt the low sample size, set the first data point somewhere close to \\((0,0)\\). It should have little effect on the correlation. Now change the seed (and tweak the point). What happens to the correlation with a new sample? Try that again! Now try it with higher sample sizes.\n\n\nWhat to do about outliers\nThere are four reasons that there might be an outlier in a data set:\n\nSomething went wrong in the data entry. Somebody left our a decimal point, or hit an extra key on the keyboard. Or maybe a number got put in the wrong column, so the subject’s shoe size was entered in place of the subject’s IQ.\nSomething went wrong in the data collection. I had a friend who used to hook research subjects up to the Vitalog monitoring pack. It included a probe for body temperature. Sometimes the probe would read 72 degrees F (room temperature) instead of 98 degrees F (body temperature). They called this “probe slippage.”\nThere is a person in the sample who really doesn’t belong there. For example, the teacher took the test along with the students, and somehow the teacher’s answers were mixed in with th student answers.\nThere is a member of population that is just a little bit different. Maybe they belong to a rare subpopulation.\n\nIf the experimenters took good records, problems of Type 1 can be corrected (also, problems of Type 3 clearly identified). If not, it may still be possible to identify that the value is clearly out of range and needs to be eliminated (for example, an SAT score of 0, when the minimum SAT score is 200). The same thing is true for errors of Type 2. When the value is clearly out of range that is the best solution, although you need to be careful that the missingness is not related to what is being studied (for example, patients dropping out of a drug trial because of the side effects).\nIn the absence of good records Type 3 and Type 4 outliers are hard to distinguish. Often what we want to do is to take the data point out and run the analysis again. Then we can compare the two correlation coefficients. If the difference is small, no problem. If the difference is big, we can report that we have an influential point and let the reader come to his or her own conclusion."
  },
  {
    "objectID": "RegressionPrediction.html",
    "href": "RegressionPrediction.html",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "RegressionPrediction.html#the-cars-data-set",
    "href": "RegressionPrediction.html#the-cars-data-set",
    "title": "Regression Prediction Error",
    "section": "",
    "text": "We will work with an example data set from Ezekiel (1930) which provides information about the speed of a number of cars and the stopping distance in feet.\n\nhelp(cars)\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nLets take a quick look at these data.\n\nplot(dist~speed,data=cars,xlab=\"Speed (mph)\", ylab=\"Stopping Distance (ft)\")\nabline(lm(dist~speed,data=cars))\nlines(lowess(cars),col=2,lty=2)\n\n\n\n\n\n\n\n\nThe solid black line is the least squares regression line, or basic model.\nThe dashed red line is a lowess curve fit to the same date.\n\nThere may be a little bit of a curve here, but it is hard to see.\n\nWe will go ahead and fit a regression using least squares. (This the the lm or linear model function in R.)\n\ncars.fit &lt;- lm (dist~speed,data=cars)\nprint(cars.fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nCoefficients:\n(Intercept)        speed  \n    -17.579        3.932  \n\n\nThe method of least squares or maximum likelihood (which in the case of simple regression are the same) finds the single best fitting line.\n\nLeast Squares means the line has the smallest some of squared residuals.\nMaximum Likelihood means that these are the parameters (slope and intercept) that have the highest probability of generating the target data."
  },
  {
    "objectID": "RegressionPrediction.html#lots-of-different-regression-lines",
    "href": "RegressionPrediction.html#lots-of-different-regression-lines",
    "title": "Regression Prediction Error",
    "section": "Lots of different regression lines",
    "text": "Lots of different regression lines\nI’ll try the regression using a different method (Markov Chain Monte Carlo, or MCMC). In this method we sample 4000 different plausible sets of parameters that could have given rise to the data. (These are sampled with a probability proportional to how likely they are to have generated the observed data).\nThe printed summary shows the median of the 4000 samples. It should be close to, but not exactly the same as, the least squares/maximum likelihood estimate.\n\n#library(rstanarm)  ## Called above\ncars.mcmc &lt;- stan_glm(dist~speed,data=cars,refresh=0)\ncars.coef &lt;- as.matrix(cars.mcmc$stanfit)\nprint(cars.mcmc)\n\nstan_glm\n family:       gaussian [identity]\n formula:      dist ~ speed\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -17.7    6.9 \nspeed         3.9    0.4 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 15.5    1.6  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "objectID": "RegressionPrediction.html#mean-confidence-interval",
    "href": "RegressionPrediction.html#mean-confidence-interval",
    "title": "Regression Prediction Error",
    "section": "Mean Confidence Interval",
    "text": "Mean Confidence Interval\nThe MCMC approach is useful because it helps us remember that the estimates that are produced by the [least squares] regression are not the truth, but rather just the most likely set of parameters. There are other possibilities that are nearly as likely.\nThe next graph is designed to show this.\nThe first N (you can adjust using the slider) samples from the MCMC are plotted as gray lines.\nThe least squares line is plotted in black.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\nNumber of plausible values to plot\n\n\n\n\n\n\n\n\n\n\nNote the dashed curves surrounding the regression line.\nThese are the confidence interval for the regression line.\nThe level of the confidence interval is how many of these plausible regression lines should fit between the dashed curves (expressed as a percentage).\nSPSS calls this the “mean” prediction interval. R calls it the “confidence” interval.\nYou can use the graph (or the R predict function, or the prediction option in SPSS) to get a prediction for the average (over a number of trials) stopping time at a given speed."
  },
  {
    "objectID": "RegressionPrediction.html#individual-prediction-interval",
    "href": "RegressionPrediction.html#individual-prediction-interval",
    "title": "Regression Prediction Error",
    "section": "Individual Prediction Interval",
    "text": "Individual Prediction Interval\nThe mean confidence interval above is for the average over many attempts at stopping the car.\nWe don’t expect a single attempt to fall exactly on the line.\n\n68% of the time we expect to be one standard error above or below the line.\n95% of the time we expect to be two standard errors above or below the line.\nTo get the total error, we need to add\n\nThe error in the regression line (see above)\nThe error around the regression line.\n\n\n(Actually, we add these on the squared variance scale).\nThe picture below shows the individual prediction interval. Once again, you can pick your confidence level.\nSPSS calls this the individual prediction interval. R calls it the prediction interval.\n\n\n\n\n\n\nConfidence Level:\n\n50\n68\n90\n95\n99\n\n\n\n\n\n\n\n\n\n\n\nLook at the area in the graph which is colored cyan.\nThese are predictions that the car will stop in negative distance. Impossible!\nThe model is wrong.\nThat shouldn’t worry us, models are always wrong. The just might be close enough to be right to be useful.\nWe might say that the linear model is useful, but only if the car is going 5 mph or more."
  },
  {
    "objectID": "RegressionPrediction.html#model-checking",
    "href": "RegressionPrediction.html#model-checking",
    "title": "Regression Prediction Error",
    "section": "Model Checking",
    "text": "Model Checking\nNote that there was a slight curve in the lowess line in the scatterplot at the top of this analysis.\nSometimes the curve is easier to see if we take the linear trend out.\nWe can do this by plotting the residuals versus the fitted values.\n In a simple regression, this is the same as plotting against \\(X\\), as the fitted values are just a linear transformation of \\(X\\) (and the graph will just be rescaled to fit). For multiple regression, the fitted values are a mix of all the \\(X\\) values, so this plot is a useful summary.\n\n\n\n\n\n\n\n\n\nLooking a little more closely, we can see the curve.\nIt would be easy to miss without the lowess line, but the lowess line points it out to us.\nThere is a little bit of curvature, curving up at the lower distances, keeping the stopping distances in positive territory.\nSo what to conclude?\n\nIn the range of 5 mph – 25 mph the linear model looks pretty good.\nFor low speeds, we need a better model.\nMaybe we need a better model for higher speeds as well."
  },
  {
    "objectID": "CorrelationExercise.html",
    "href": "CorrelationExercise.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\n\n\n\n\n\n\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "VaccineCI.html",
    "href": "VaccineCI.html",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "",
    "text": "Data from Moderna Vaccine Study Control Group\nX &lt;- 95\nN &lt;- 1500\nIn a sample of 1500 volunteers receiving the placebo, there were 95 positive cases; so our estimate for the rate of COVID-19 in this population (and this time period) is 0.063.\nRecall that the formula for the \\(\\alpha\\) confidence interval is\nC.I. \\[ \\bar X \\pm (z_{1-\\alpha/2})\\ \\sigma_{\\bar X}\\] Here \\(z_{1-\\alpha/2}\\) is the \\(1-\\alpha/2\\) quantile of the normal distribution. We can look that up on a Normal Table. For \\(\\alpha=.95\\), \\(z_{1-\\alpha/2}=1.96\\approx 2\\).\nFor binomial distribution \\[ \\bar X = X/N=\\hat p\\] This is the value 0.063 we calculated earlier. The hat over the \\(p\\) is a sign that it is a (maximum likelihood) estimate.\nThe usual formula for the standard error of a mean (from a simple random sample) is \\[ \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{N}} \\] For the binomial distribution, the standard deviation is \\[ \\sigma = \\sqrt{p(1-p)}; \\qquad s = \\sqrt{\\hat p(1-\\hat p)}\\] Plug that into the formula for the standard error and we get:\n\\[ \\sigma_{\\bar X} = \\sqrt{p(1-p)/N} \\] Lets go ahead and calculate those\np.hat &lt;- X/N\nse &lt;- sqrt(p.hat*(1-p.hat)/N)\nThe probability estimate is 0.063 and the standard error is 0.0063.\nI’ll now use an R trick. qnorm() is the R function to calculate the quantiles of the normal distribution. If I give it two probabilities, it will give me both the postive and negative values. So I will pass it \\((\\alpha/2,1-\\alpha/2)\\), this gives the values \\(r round(qnorm(c(.025,.975)),3)\\).\nBecause R does calculations on vectors, it will calculate both sides of the confidence interval with one formula.\nci &lt;- p.hat + qnorm(c(.025,.975))*se\nPrevlance of covid at the time and in the locations the study was run was between (5.1%,7.6%).\nNote that a lot of things have changed between now and then. In particular, the rise of the much more transmissable delta variant. But also changes in how seriously people take masking and other percautions. In particular, there is probably considerable regional variation in the prevalence of COVID-19.\nThe web site https://www.microcovid.org/ tracks this on a county-by-county basis."
  },
  {
    "objectID": "VaccineCI.html#severe-covid",
    "href": "VaccineCI.html#severe-covid",
    "title": "Confidence Interval: COVID Vaccine tests",
    "section": "Severe Covid",
    "text": "Severe Covid\nSame thing with the severe (hospitalizations or death) COVID numbers.\n\nX1 &lt;- 11\np1 &lt;- X1/N\nse1 &lt;- sqrt(p1*(1-p1)/N)\nci1 &lt;- p1 + qnorm(c(.025,.975))*se1\n\nPrevlance of severe covid at the time and in the locations the study was run was between (0.3%,1.2%)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "SkewnessPractice.html",
    "href": "SkewnessPractice.html",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "SkewnessPractice.html#skewness-determination-exercise.",
    "href": "SkewnessPractice.html#skewness-determination-exercise.",
    "title": "Skewness Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Each will be plotted with a normal curve on top for reference. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "LawOfLargeNumbersAnimated.html",
    "href": "LawOfLargeNumbersAnimated.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n(Skip this unless you are good with calculus.) This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "LawOfLargeNumbersAnimated.html#a-demonstration.",
    "href": "LawOfLargeNumbersAnimated.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "href": "LawOfLargeNumbersAnimated.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "href": "LawOfLargeNumbersAnimated.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also the non-animated version."
  },
  {
    "objectID": "CentralLimitTheroem.html",
    "href": "CentralLimitTheroem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Pick a distribution: * Uniform – platykurtic * Binomial – symmetric and mesokurtic * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the normal distribution function and density.\nThe left column shows the original distribution. (I call that the black hat in my CLT demo.)\nThe right column shows the distribution of means of size \\(M\\) (adjusted with the second slider). (This is the white hat distribuiton, and \\(M\\) is the number of cards averaged to get the white hat value.)\nThe top row shows histograms with a normal curve on top.\nThe bottom row shows a QQ-plot. This shows how much the sample is different from a normal distribution. A normal distribution should be right on top of the diagonal line.\nAs \\(M\\) (the number of cards averages to get to the white hat) gets bigger, the distribution should get closer and closer to the normal distribution."
  },
  {
    "objectID": "CentralLimitTheroem.html#take-home",
    "href": "CentralLimitTheroem.html#take-home",
    "title": "Central Limit Theorem",
    "section": "Take home",
    "text": "Take home\n\nEven if the underlying data aren’t normal, the distribuiton of the means of various groups should be close to normal.\nClose depends on the sample size.\nA bigger sample is needed if the data are highly skewed (expontential and gamma) or leptokurtic (exponential and Student t)."
  },
  {
    "objectID": "CorrelationExamples.html",
    "href": "CorrelationExamples.html",
    "title": "Scatterplot examples",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a [random seed][seed] and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "CorrelationExamples.html#mostly-linear",
    "href": "CorrelationExamples.html#mostly-linear",
    "title": "Scatterplot examples",
    "section": "Mostly linear",
    "text": "Mostly linear\nThis happens when we have a moderately high to strong correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "CorrelationExamples.html#blobby-elipse",
    "href": "CorrelationExamples.html#blobby-elipse",
    "title": "Scatterplot examples",
    "section": "Blobby Elipse",
    "text": "Blobby Elipse\nAs the correlation coefficient gets lower, the scatterplot looks more blobby, but you can still tell that there is a slope. This is a weak to moderate correlation.\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "CorrelationExamples.html#no-relationship",
    "href": "CorrelationExamples.html#no-relationship",
    "title": "Scatterplot examples",
    "section": "No Relationship",
    "text": "No Relationship\nNot much is going on here. One thing that confuses people is the idea that linear regression doesn’t work here. Actually, it gives a quite accurate picture: it tells you that not much is going on, which is what is actually happening. The prediction from the regression will be that \\(\\bar Y\\) is the best predicted value for \\(Y\\).\n\n\n\n\n\n\nCorrelation Coefficient:\n\n\n\n\n\n\n\n\nNegative Correlation"
  },
  {
    "objectID": "CorrelationExamples.html#curve",
    "href": "CorrelationExamples.html#curve",
    "title": "Scatterplot examples",
    "section": "Curve",
    "text": "Curve\nA curved relationship doesn’t look like a line.\nConsider a quadradic relationship: \\[ Y = b_2 X^2 + b_1 X + b_0 + \\epsilon\\] This is a multiple (or quadradic) regression. You can adjust the coefficients in the plot below.\n\n\n\n\n\n\nQuadradic Term Slope:\n\n\n\n\n\nLinear Term Slope:\n\n\n\n\n\nIntercept:\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nIf we try to run a linear regression when the relationship is curved, it will only tell us part of the story. The story it will tell is the red line, and not the blue curve."
  },
  {
    "objectID": "CorrelationExamples.html#broken-lines",
    "href": "CorrelationExamples.html#broken-lines",
    "title": "Scatterplot examples",
    "section": "Broken Lines",
    "text": "Broken Lines\nSometimes the reltionship changes somewhere through the range of the data. Often this is a ceiling effect: the effect of \\(X\\) on \\(Y\\) hits a ceiling. For example, in the first couple of years of teaching, the ability of new teachers rises very rapidly as they gain experience. But after 3–5 years, the effect levels out and the teachers grow much more slowly.\nIdeally we would fit two linear regression to these data splitting at a certain value of \\(X\\), \\(x_0\\). So,\n\\[ Y = \\begin{cases}\nb_{11} X + b_{01} + \\epsilon & \\text {when} X \\leq x_0 \\\\\nb_{12} X + b_{02} + \\epsilon & \\text {when} X \\ge x_0\n\\end{cases}\n\\]\n\n\n\n\n\n\nFirst Slope:\n\n\n\n\n\nSecond Slope:\n\n\n\n\n\nCrossover Point (x[0])\n\n\n\n\n\nError Standard Deviation:\n\n\n\n\n\n\n\n\n\n\nCheck out this page to practice identifying these."
  },
  {
    "objectID": "SkewnessQQ.html",
    "href": "SkewnessQQ.html",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "SkewnessQQ.html#skewness-determination-exercise.",
    "href": "SkewnessQQ.html#skewness-determination-exercise.",
    "title": "Skewness Practice with Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; positively skewed distributions will yield a ‘U’ shaped curve, and negatively skewed distributions a ‘C’ shaped curve. (An ‘S’ or ‘Z’ shaped curve indicates kurtosis, not skewness). Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size. (Bigger sample sizes are easier.)\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "SlopeStandardErrors.html",
    "href": "SlopeStandardErrors.html",
    "title": "Slope Variation",
    "section": "",
    "text": "library(shiny)\nn &lt;- 1000\nr &lt;- .6\nx &lt;- rnorm(n)\ny &lt;- r*x + sqrt(1-r^2)*rnorm(n)\ndat &lt;- data.frame(x,y)\n\nfit1 &lt;- lm(y~x,data=dat)\n\nplot(x,y,type=\"n\")\ncoef.sim &lt;- coef(sim(fit1))\nfor (i in 1:nrow(coef.sim)) {\n  abline(a=coef.sim[i,1],b=coef.sim[i,2],col=\"gray50\")\n}\npoints(x,y)\nabline(fit1)\n\n\n\n\n\n\n\nplot(coef.sim)\n\n\n\n\n\n\n\ncor(coef.sim)\n\n            (Intercept)          x\n(Intercept)  1.00000000 0.03692601\nx            0.03692601 1.00000000"
  },
  {
    "objectID": "NormalParams.html",
    "href": "NormalParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "NormalParams.html#inputs-and-outputs",
    "href": "NormalParams.html#inputs-and-outputs",
    "title": "Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\n  inputPanel(\n  sliderInput(\"mn\", label = \"Mean:\",\n              min=0, max=100, value=50, step=1),\n  \n  sliderInput(\"sd\", label = \"Standard Deviation:\",\n              min = 0.2, max = 25, value = 10, step = 0.1)\n),\n,\nmainPanel(\n  plotOutput(\"normcurve\")))\n\nserver &lt;- function (input,output) {\n  output$normcurve &lt;- \n  renderPlot({\n  mn &lt;- as.numeric(input$mn)\n  sd &lt;- as.numeric(input$sd)\n  curve(dnorm(x,mn,sd),xlim=c(0,100),ylim=c(0,.1),\n        main=paste(\"Normal distribution with mean\",mn,\n                   \"and standard deviation\",sd),\n        xlab=\"X\",ylab=\"Density\")\n\n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "NormalParams.html#scale-and-location-parameters",
    "href": "NormalParams.html#scale-and-location-parameters",
    "title": "Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it stretches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphics packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui1 &lt;- fluidPage(\ninputPanel(\n  sliderInput(\"mn1\", label = \"Mean:\",\n              min=0, max=100, value=50, step=1),\n  \n  sliderInput(\"sd1\", label = \"Standard Deviation:\",\n              min = 0.2, max = 25, value = 10, step = 0.1)\n),\n mainPanel(plotOutput(\"normcurve1\")))\n \nserver1 &lt;- function(input,output) {\n output$normcurve1 &lt;- renderPlot({\n  mn1 &lt;- as.numeric(input$mn1)\n  sd1 &lt;- as.numeric(input$sd1)\n  curve(dnorm(x,mn1,sd1),xlim=c(mn1-3*sd1,mn1+3*sd1),\n        main=paste(\"Normal distribution with mean\",mn1,\n                   \"and standard deviation\",sd1),\n        xlab=\"X\",ylab=\"Density\")\n\n})\n}\nshinyApp(ui1, server1)"
  },
  {
    "objectID": "TestCI.html",
    "href": "TestCI.html",
    "title": "Confidence Intervals and Tests",
    "section": "",
    "text": "Suppose we are trying to find out the mean of a certain population, \\(\\mu\\). For example, suppose we are interested in the game eRebuild (https://mileresearch.coe.fsu.edu/erebuild) which aims to teach middle school students mathematics. Here \\(\\mu\\) would be how much a math a middle school student learns by playing the game. That is our target of inference.\nWe will make three simplifying assumptions:\n\nWe can measure “math”"
  },
  {
    "objectID": "Chi2calculator.html",
    "href": "Chi2calculator.html",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(x^2 &lt; X^2)\"=\"upper\",\n                          \"Lower tail: Pr(X^2 &lt; x^2)\"=\"lower\"),\n              selected = \"upper\"),\n  \n  numericInput(\"x2\", label = \"chi-squared value:\", value=2),\n  numericInput(\"df\", label = \"Degrees of Freedom\", value =1)\n),\nmainPanel(\n  plotOutput(\"probplot\")))\n\nserver &lt;- function (input,output) {\n  output$probplot &lt;- renderPlot({\n  q &lt;- input$x2\n  df &lt;- input$df\n  p &lt;- switch(input$tails,\n              upper=1-pchisq(q,df),\n              lower=pchisq(q,df))\n  xl &lt;- round(qchisq(.999,df),1)\n  curve(dchisq(x,df),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"chi-squared = \",round(q,3)),\n        xlim = c(0,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"Chi-squared\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dchisq(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(0,seq(0,q,0.01),q)\n           cord.yl &lt;- c(0,dchisq(seq(0,q,0.01),df),0)\n           if(!is.finite(cord.yl[2])) cord.yl[2] &lt;- cord.yl[3]\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         })\n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "Chi2calculator.html#chi-squared-probabilities.",
    "href": "Chi2calculator.html#chi-squared-probabilities.",
    "title": "Chi-squared Calculator",
    "section": "",
    "text": "In this tool, you input a \\(\\chi^2\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(x^2 &lt; X^2)\"=\"upper\",\n                          \"Lower tail: Pr(X^2 &lt; x^2)\"=\"lower\"),\n              selected = \"upper\"),\n  \n  numericInput(\"x2\", label = \"chi-squared value:\", value=2),\n  numericInput(\"df\", label = \"Degrees of Freedom\", value =1)\n),\nmainPanel(\n  plotOutput(\"probplot\")))\n\nserver &lt;- function (input,output) {\n  output$probplot &lt;- renderPlot({\n  q &lt;- input$x2\n  df &lt;- input$df\n  p &lt;- switch(input$tails,\n              upper=1-pchisq(q,df),\n              lower=pchisq(q,df))\n  xl &lt;- round(qchisq(.999,df),1)\n  curve(dchisq(x,df),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"chi-squared = \",round(q,3)),\n        xlim = c(0,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"Chi-squared\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dchisq(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(0,seq(0,q,0.01),q)\n           cord.yl &lt;- c(0,dchisq(seq(0,q,0.01),df),0)\n           if(!is.finite(cord.yl[2])) cord.yl[2] &lt;- cord.yl[3]\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         })\n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "Chi2calculator.html#chi-square-quantiles-critical-values.",
    "href": "Chi2calculator.html#chi-square-quantiles-critical-values.",
    "title": "Chi-squared Calculator",
    "section": "Chi-square Quantiles (Critical values).",
    "text": "Chi-square Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding \\(X^2\\) score.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui1 &lt;- fluidPage(\ninputPanel(\n   selectInput(\"tails1\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(x^2 &lt; X^2)\"=\"upper\",\n                          \"Lower tail: Pr(X^2 &lt; x^2)\"=\"lower\"),\n              selected = \"upper\"),\n  numericInput(\"pp\", label = \"Probability of shaded region:\", value=0.05, min=0, max=1),\n  numericInput(\"df1\", label = \"Degrees of Freedom\", value =1)\n),\nmainPanel(\n  plotOutput(\"quantplot\")))\n\nserver1 &lt;- function (input,output) {\n  output$quantplot &lt;- renderPlot({\n  pp &lt;- as.numeric(input$pp)\n  df1 &lt;- as.numeric(input$df1)\n  qq &lt;- switch(input$tails1,\n              upper=qchisq(1-pp,df1),\n              lower=qchisq(pp,df1))\n xl1 &lt;- round(qchisq(.999,df1),1)\n  curve(dchisq(x,df1),main=paste(\"Probability of shaded region = \",round(pp,3)),\n        sub=paste(\"X2(\",df1,\") = \",round(qq,3)),\n        xlim = c(0,xl1),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"Chi-squared\")\n   switch(input$tails,\n         upper={\n           cord.xu1 &lt;- c(qq,seq(qq,xl1,0.01),xl1)\n           cord.yu1 &lt;- c(0,dchisq(seq(qq,xl1,0.01),df1),0)\n           polygon(cord.xu1,cord.yu1,col='plum')\n           axis(1,qq,paste(round(qq,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl1 &lt;- c(0,seq(0,qq,0.01),q)\n           cord.yl1 &lt;- c(0,dchisq(seq(0,qq,0.01),df1),0)\n           if(!is.finite(cord.yl[2])) cord.yl[2] &lt;- cord.yl[3]\n           polygon(cord.xl1,cord.yl1,col='plum')\n           axis(1,qq,paste(round(qq,3)),cex.axis=2)\n         })\n\n})\n}\nshinyApp(ui=ui1,server=server1)\nFor \\(\\chi^2\\) tests, one almost always looks at upper tail, and rarely look at the lower tail (does the data fit better than expected). Two-tailed tests are never (“What never?”, “No Never!”, “What Never?”, “Well hardly ever!”, Gilbert & Sulivan, HMS Pinfore) done."
  },
  {
    "objectID": "Studenttcalculator.html",
    "href": "Studenttcalculator.html",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(t &lt; T)\"=\"upper\",\n                          \"Lower tail: Pr(T &lt; t)\"=\"lower\",\n                          \"Both tails: Pr(T &lt;-t or t&lt; T)\"=\"both\",\n                          \"Middle: Pr(-t &lt; T &lt; t)\"=\"middle\"),\n              selected = \"both\"),\n  \n  numericInput(\"z\", label = \"t-value:\", value=2),\n  numericInput(\"df\", label = \"Degrees of Freedom (n-1 or n1 + n2 -2)\", value =30)\n),\nmainPanel(\n  plotOutput(\"tcurve\")))\n\nserver &lt;- function (input,output) {\n  output$tcurve &lt;- \nrenderPlot({\n  q &lt;- input$z\n  df &lt;- input$df\n  p &lt;- switch(input$tails,\n              upper=1-pt(q,df),\n              lower=pt(q,df),\n              both=2*pt(-abs(q),df),\n              middle=1-2*pt(-abs(q),df))\n  xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n  curve(dt(x,df),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"t = \",round(q,3)),\n        xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"t\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q)\n           cord.yl &lt;- c(0,dt(seq(-xl,q,0.01),df),0)\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         both={\n           q &lt;- abs(q)\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q)\n           cord.yl &lt;- c(0,dt(seq(-xl,-q,0.01),df),0)\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         },\n         middle={\n           q &lt;- abs(q)\n           cord.xmid &lt;- c(-q,seq(-q,q,0.01),q)\n           cord.ymid &lt;- c(0,dt(seq(-q,q,0.01),df),0)\n           polygon(cord.xmid,cord.ymid,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         })\n  \n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "Studenttcalculator.html#students-t-probabilities.",
    "href": "Studenttcalculator.html#students-t-probabilities.",
    "title": "Student’s Calculator",
    "section": "",
    "text": "In this tool, you input a \\(t\\) score and the degrees of freedom, and get a corresponding \\(p\\)-value.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(t &lt; T)\"=\"upper\",\n                          \"Lower tail: Pr(T &lt; t)\"=\"lower\",\n                          \"Both tails: Pr(T &lt;-t or t&lt; T)\"=\"both\",\n                          \"Middle: Pr(-t &lt; T &lt; t)\"=\"middle\"),\n              selected = \"both\"),\n  \n  numericInput(\"z\", label = \"t-value:\", value=2),\n  numericInput(\"df\", label = \"Degrees of Freedom (n-1 or n1 + n2 -2)\", value =30)\n),\nmainPanel(\n  plotOutput(\"tcurve\")))\n\nserver &lt;- function (input,output) {\n  output$tcurve &lt;- \nrenderPlot({\n  q &lt;- input$z\n  df &lt;- input$df\n  p &lt;- switch(input$tails,\n              upper=1-pt(q,df),\n              lower=pt(q,df),\n              both=2*pt(-abs(q),df),\n              middle=1-2*pt(-abs(q),df))\n  xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n  curve(dt(x,df),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"t = \",round(q,3)),\n        xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"t\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q)\n           cord.yl &lt;- c(0,dt(seq(-xl,q,0.01),df),0)\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         both={\n           q &lt;- abs(q)\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df),0)\n           polygon(cord.xu,cord.yu,col='plum')\n           cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q)\n           cord.yl &lt;- c(0,dt(seq(-xl,-q,0.01),df),0)\n           polygon(cord.xl,cord.yl,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         },\n         middle={\n           q &lt;- abs(q)\n           cord.xmid &lt;- c(-q,seq(-q,q,0.01),q)\n           cord.ymid &lt;- c(0,dt(seq(-q,q,0.01),df),0)\n           polygon(cord.xmid,cord.ymid,col='plum')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         })\n  \n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "Studenttcalculator.html#students-t-quantiles-critical-values.",
    "href": "Studenttcalculator.html#students-t-quantiles-critical-values.",
    "title": "Student’s Calculator",
    "section": "Student’s t Quantiles (Critical values).",
    "text": "Student’s t Quantiles (Critical values).\nIn this tool, you input a probability and degrees of freedom, and get a corresponding t score.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui1 &lt;- fluidPage(\n  inputPanel( \n    selectInput(\"tails1\", label = \"Which tails\", \n                choices = c(\"Upper tail: Pr(t &lt; T)\"=\"upper\",\n                            \"Lower tail: Pr(T &lt; t)\"=\"lower\", \n                            \"Both tails: Pr(T &lt;-t or t&lt; T)\"=\"both\", \n                            \"Middle: Pr(-t &lt; T &lt; t)\"=\"middle\"),\n                selected = \"both\"),\n\n  numericInput(\"p\", label = \"Probability of shaded region:\", \n               value=0.05, min=0, max=1), \nnumericInput(\"df1\", label = \"Degrees of Freedom (n-1 or n1 + n2 -2)\", \n             value =30) ),\nmainPanel(\n  plotOutput(\"tcurve1\")))\n\nserver1 &lt;- function (input,output) {\n  output$normcurve &lt;- \nrenderPlot({ \n  pp &lt;- input$p\n  df1 &lt;- input$df1 \n  q &lt;- switch(input$tails1,\n              upper=qt(1-pp,df1),\n              lower=qt(pp,df1),\n              both=qt(1-pp/2,df1),\n              middle=qt(.5+pp/2,df1))\n  xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n  curve(dt(x,df1),main=paste(\"Probability of shaded region = \",round(pp,3)),\n        sub=paste(\"t(\",df1,\") = \",round(q,3)),\n        xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"t\")\n  switch(input$tails1, \n         upper={ cord.xu &lt;- c(q,seq(q,xl,0.01),xl) \n         cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df1),0) \n         polygon(cord.xu,cord.yu,col='plum')\n         axis(1,q,paste(round(q,3)),cex.axis=2) }, \n         lower={ cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q) \n          cord.yl &lt;- c(0,dt(seq(-xl,q,0.01),df1),0)\n          polygon(cord.xl,cord.yl,col='plum')\n          axis(1,q,paste(round(q,3)),cex.axis=2) }, \n         both={ q &lt;- abs(q) \n         cord.xu &lt;- c(q,seq(q,xl,0.01),xl) \n         cord.yu &lt;- c(0,dt(seq(q,xl,0.01),df1),0) \n         polygon(cord.xu,cord.yu,col='plum') \n         cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q) \n         cord.yl &lt;- c(0,dt(seq(-xl,-q,0.01),df1),0) \n         polygon(cord.xl,cord.yl,col='plum') \n         axis(1,q,paste(round(q,3)),cex.axis=2)\n         axis(1,-q,paste(-round(q,3)),cex.axis=2) }, \n         middle={ q &lt;- abs(q) \n         cord.xmid &lt;- c(-q,seq(-q,q,0.01),q) \n         cord.ymid &lt;- c(0,dt(seq(-q,q,0.01),df1),0)\n         polygon(cord.xmid,cord.ymid,col='plum')\n         axis(1,q,paste(round(q,3)),cex.axis=2)\n         axis(1,-q,paste(-round(q,3)),cex.axis=2) \n         })\n\n})\n}\nshinyApp(ui=ui1,server=server1)"
  },
  {
    "objectID": "PoissonParms.html",
    "href": "PoissonParms.html",
    "title": "Poisson Params",
    "section": "",
    "text": "The Poisson distribution is a distribution for counts of events.\nAssume the following things:\n\nEvents happen at a rate \\(\\lambda\\) per unit interval on average.\nCount the number of events in a time interval of \\(T\\) units.\nAssume that the events happen at a uniform rate throughout the interval (e.g., we don’t get more customers in the morning than the afternoon).\n\nThe the number of events, \\(X\\), follows a Poisson distribution.\n\\[P(X=x) = \\frac{(\\lambda T)^x}{x!}e^{-\\lambda T}\\] The distribution looks like:\n\n\n\n\n\n\nExpected number of events per unit time\n\n\n\n\n\nTime Interval:\n\n\n\n\n\n\n\n\n\n\nThe mean and variance of the Poisson distribution are \\(\\lambda T\\) and \\(\\lambda T\\).\nAs the variance grows pretty quickly, statisticians will often take the square root of count data (especially if there is heteroscedasticity) to stabilize the variance."
  },
  {
    "objectID": "KurtosisPractice.html",
    "href": "KurtosisPractice.html",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "KurtosisPractice.html#kurtosis-determination-exercise.",
    "href": "KurtosisPractice.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. A normal curve is drawn over the top for reference.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "LawOfLargeNumbers.html",
    "href": "LawOfLargeNumbers.html",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "This is pretty close to the frequency definition of probability. Suppose the probability of some event is \\(p\\). Suppose further than we sample \\(N\\) times from the process that generates this event. Let \\(p_N\\) be the proportion of times the event occurs in \\(N\\) trials. As \\(N\\) gets bigger and bigger, \\(p_N\\) gets closer and closer to \\(p\\).\n(Skip this unless you are good with calculus.) This is one of those epsilon-delta theorems. So let \\(\\delta\\) be a difference from \\(p\\) and let \\(\\epsilon\\) be a small probability. For any \\(\\epsilon\\) and \\(\\delta\\), there exists an \\(N\\) such that \\(P(|p_N-p|&gt;\\delta) &lt; \\epsilon\\).\n\n\nIn the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\).\n\n\n\nPick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "LawOfLargeNumbers.html#a-demonstration.",
    "href": "LawOfLargeNumbers.html#a-demonstration.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "In the picture below, pick a probability \\(p\\) and a sample size \\(N\\). The computer will generate samples up to \\(N\\) and plot \\(p_N\\).\nThe \\(\\delta\\)-line is an error bound plus or minus \\(\\delta\\) units from the target \\(p\\). This is a target so you can judge how close you got.\n\n\n\n\n\n\nMaximum Sample Size:\n\n50\n100\n200\n500\n1000\n\n\n\n\n\n\nProbability of event (p)\n\n\n\n\n\nDistance of reference line from target (delta)"
  },
  {
    "objectID": "LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "href": "LawOfLargeNumbers.html#convergence-of-distributions-boot-strap-distribution",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "We can use the Law of Large Numbers to prove an important theorem. As the sample size gets larger and larger, the sample looks more and more like the population it is drawn from.\n Technically, the Law of Large Numbers refers to the result above. But we can use it so show a very important basis of statistics. Suppose we have some kind of distribution, \\(F(x)\\), that generates numbers, \\(X\\). Recall that the definition of \\(F(x)=\\Pr(X \\leq x)\\).\n Draw a sample of size \\(N\\) from this distribution. Now consider the sampled data points \\(X_1,\\ldots,X_N\\), and consider sampling a new value \\(Y\\) from that distribution. Let \\(F_N(y) = \\Pr(Y \\leq y)\\). This is sometimes called the bootstrap distribution.\n By the law of large numbers, for every \\(y\\), as \\(N\\) gets large \\(F_N(y) \\rightarrow F(y)\\). So the sample distribution \\(F_N()\\) converges to the \\(F()\\)."
  },
  {
    "objectID": "LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "href": "LawOfLargeNumbers.html#demonstration-of-convergence-of-distributions.",
    "title": "Law of Large Numbers",
    "section": "",
    "text": "Pick a distribution: * Normal – standard normal * Exponential – highly skewed * Gamma (shape = 3) – skewed * T (df =3) – high kurtosis\nSlide the sample size up and down, notice how the empirical distribution function and histogram coverge to the theoretical distribution function and density.\n\n\n\n\n\n\nDistribution Type\n\nNormal\nExponential\nGamma\nT\n\n\n\n\n\n\nMaximum Sample Size:\n\n\n\n\n\n\n\n\n\n\nSee also the animated version."
  },
  {
    "objectID": "Z-scores.html",
    "href": "Z-scores.html",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "Z-scores.html#standardizing-a-raw-score.",
    "href": "Z-scores.html#standardizing-a-raw-score.",
    "title": "Standardized Variables",
    "section": "",
    "text": "A (interval or ratio) variable on a raw score can be standardized to have mean 0 and standard deviation 1 by simply subtracting the mean and dividing by the standard deviation. This formula come in two flavors: one using the population mean and standard deviation (mu and sigma) and one using the sample statistics (x-bar and s). The subscripts are to remind you what variable you are using, as there is often both an X and Y wandering around.\n\\[ z = \\frac{x-\\mu_X}{\\sigma_X}; \\qquad Z = \\frac{X-\\bar X}{s_X} \\]\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nx:\n\n\n\n\n\n\n\n\n\n\nOften the next step is to look up the Z score on a normal calculator."
  },
  {
    "objectID": "Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "href": "Z-scores.html#going-from-a-standard-z-score-to-a-raw-score.",
    "title": "Standardized Variables",
    "section": "Going from a standard (z) score to a raw score.",
    "text": "Going from a standard (z) score to a raw score.\nSolving the above equations for X allows the z-score to be translated back into a raw score. Often, a new variable is needed, so lets change the variables from X to Y. Once again, there are two variants based on whether sample or population means and standard deviations are used:\n\\[ y = \\sigma_Y z + \\mu_Y\\, ; \\qquad Y = s_Y Z + \\bar{Y}\\ .\\]\n\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nz:\n\n\n\n\n\n\n\n\n\n\n\n\nNote that these formulae are well worth memorizing, as they will come up over and over again."
  },
  {
    "objectID": "Independence.html",
    "href": "Independence.html",
    "title": "Independence",
    "section": "",
    "text": "Imagine a population which is split into two groups: \\(A\\) and \\(B\\). We select 100 people at random and ask them a question, which has two answers yes and no. Define the following quantities:\nDefine the following values (row and column totals):\nDividing any of those numbers by \\(N_{xx}\\) produces a corresponding proportion \\(P_{xx}\\) (which can be interpreted as a probability or proportion.\nSuppose group membership and the answer to the question are statistically indepedent. In the diagram below, adjust \\(P_{A+}\\) and \\(P_{+y}\\) to make a two-by-two table:\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\nThere are two things you should notice about the independent data.\nWe could say that the row and column proportions are always the same.\nAnother way to think about this is to say: * If we learned which group a person belongs to, that would not change the probability of their answer. * If we learned how a person answered, that would not change the probablity of their group."
  },
  {
    "objectID": "Independence.html#dependent",
    "href": "Independence.html#dependent",
    "title": "Independence",
    "section": "Dependent",
    "text": "Dependent\nTo make the table dependence, we need to add another parameter to the model to specify the degree of dependence.\nFor a two-by-two table, the odds ratio is as fairly easy to understand choice: \\[ OR = \\frac{P_{Ay}/P_{An}}{P_{By}/P_{Bn}}\\] When group and answer are indpendent the cross product ratio should be 1.\nIf Group \\(A\\) is more likely to answer yes, then the ratio should be bigger than 1.\nIf Group \\(B\\) is more likely to answer yes, then the ratio should be less than one.\n\n\n\n\n\n\nP(Member of Group A)\n\n\n\n\n\nP(Answered `yes`)\n\n\n\n\n\nOdds Ratio\n\n1/4\n1/3\n1/2\n2/3\n1\n3/2\n2\n3\n4"
  },
  {
    "objectID": "EffectSize.html",
    "href": "EffectSize.html",
    "title": "Effect Size Calculator",
    "section": "",
    "text": "If the units of a test are well known, the size of an effect is pretty easy to understand. For example, if a study found that on average, people on this diet lost about 5 lbs (2.5 kg) in a week, most people would know what that means.\nIn many other cases, the size of the statistic depends on the measure used to determine it. For example, if a researcher finds that students on average gain 5 points on a math test after playing a mathematical game, is that a big gain or a small gain? Unlike the weight loss experiment, we don’t have the experience with that test to judge.\nThere are a number of measures that can be used to put the size of the effect into perspective. Jacob Cohen proposed dividing the difference by the population standard deviation: \\[ d = \\frac{\\mu_1 - \\mu_0}{\\sigma} .\\] This has the advantage of putting things on a readily apparent scale. So, suppose in the example of the math game above, the effect size was \\(d=.1\\). If the game was just a short thing that took an hour, that would be a pretty big deal. On the other hand, if the students needed to play all year to get that effect, it might not be so good."
  },
  {
    "objectID": "EffectSize.html#simple-case-one-group",
    "href": "EffectSize.html#simple-case-one-group",
    "title": "Effect Size Calculator",
    "section": "Simple Case – One Group",
    "text": "Simple Case – One Group\nWhen we are looking at a single group, the definition of the effect size is fairly simple. The single variable usually is a difference score; e.g., posttest - pretest. The standard deviation of interest is the standard deviation of the population of difference scores (estimated by the sample).\n\n\n\n\n\n\nMean Difference:\n\n\n\n\n\nStandard Deviation of Difference:"
  },
  {
    "objectID": "EffectSize.html#complex-case-two-groups",
    "href": "EffectSize.html#complex-case-two-groups",
    "title": "Effect Size Calculator",
    "section": "Complex Case – Two Groups",
    "text": "Complex Case – Two Groups\nConceptually, the two group case is just as simple. The numerator is the difference between the group means. The denominator is the standard deviation of the population. There is a problem: we don’t have one population, we have two: Group 1 and Group 2. What to do?\nThe answer is to take an average. Actually, we take the average of the variances, and then take the square root. There is one further complication, the two groups might have different sizes. In this case, we take a weighted average, weighting by the degrees of freedom (sample size -1). Here is the formula: \\[ \\sigma^2_{pooled} = \\frac{(N_1 -1)\\sigma^2_1 +(N_2 -1)\\sigma^2_2}{N_1+N_2-1} ,\\] take the square root of that to get the standard deviation.\nAlthough one could calculate that by hand, the calculator below will do the job for you, and then calculate the effect size at the same time. Note that the pooled SD should always be in between the SDs of group 1 and group 2. For a rough and ready estimate, you could just take the number halfway between the two.\n\n\n\n\n\n\n\nStatistics for Group 1 (experimental/focal)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\nStatistics for Group 2 (control/reference)\n\nMean:\n\n\n\nSD:\n\n\n\nN:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to do these calculations away from the internet, you can download this Excel spreadsheet which has the formula baked in: https://pluto.coe.fsu.edu/effectSize_d.xls\n\nHow big is big? Cohen’s guide to effect sizes.\nReally, the answer is entirely discipline specific. In Physics, it is quite common to be able to exert large forces and to measure very accurately, and hence be able to get very large effect sizes. In the social sciences and education, it often hard to control all of the variables that might affect the outcome, so the typical effect sizes are quite small.\nJacob Cohen presented a guideline for use in power analyses. This was really just for when you had no idea of what the size of the effect would be. He suggested:\n\n\n\nEffect\nd\n\n\n\n\nSmall\n.2\n\n\nMedium\n.5\n\n\nLarge\n.8\n\n\n\nHowever, these are really not designed for interpreting effects. There instead, you should compare to what other similar interventions are achieving.\nDylan Wiliams suggests that for educational applications, you might try dividing by the effect size of a year’s growth for that grade level. This changes rapidly with 1st graders growing nearly 2 SDs while a years worth of growth for a high school student is closer to .5 SDs. In high school, an effect size of .25 would be half a years growth, which is considerable.\n(Of course to measure small effects, you also need a very sensitive instrument, which in education means a longer test; the cost of testing is often prohibitive.)"
  },
  {
    "objectID": "KurtosisBoxplots.html",
    "href": "KurtosisBoxplots.html",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "href": "KurtosisBoxplots.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "KurtosisBoxplots.html#what-to-look-for",
    "href": "KurtosisBoxplots.html#what-to-look-for",
    "title": "Kurtosis Boxplot Practice",
    "section": "What to look for",
    "text": "What to look for\n\nIn a normal distribution the whiskers extend about 1.5 box-lengths (IQR)s from the hinges (sides of the box).\n\nIf the box is long compared to the whiskers, this is a sign the distribuiton is platykurtic.\nIf the box is short compared to the whiskers, this is a sign the distribution is leptokurtic.\n\nThe whiskers only extend to the farthest data point within 1.5 IQRs from the box. So if there is high kurtosis, this will show up as lots of outliers.\n\nWith a normal distribution, there is often 1–2 outliers per 100 data points. Much more than that is a sign of high kurtosis.\n\nIs the length of the box (IQR) long compared to the length of the whiskers?"
  },
  {
    "objectID": "KurtosisBoxplots.html#related-pages",
    "href": "KurtosisBoxplots.html#related-pages",
    "title": "Kurtosis Boxplot Practice",
    "section": "Related Pages:",
    "text": "Related Pages:\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "GammaParams.html",
    "href": "GammaParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "The exponential distribution is a distribution often used for waiting times. Suppose the expected time to the next arrival is \\(\\theta\\). Then the probability that person will come at exactly time \\(x\\) is \\(f(x|\\theta) = \\frac{1}{\\theta}e^{-x/\\theta}\\). The exponential distribution has some interesting properties. In particular, if you have already waited for time period \\(z\\), then the conditional expectation is \\(z+\\theta\\).\nSuppose instead of waiting for one event, we wait for \\(k\\) events. Then we get the gamma distribution with shape parameter \\(k\\) and scale parameter \\(\\theta\\). Its probability density function is: \\[ f(x|k,\\theta) = \\frac{1}{\\Gamma(k)\\theta^k}x^{k-1}e^{-x/\\theta}\\]\nThe expected value is \\(k\\theta\\) and the standard deviation is \\(k\\theta^2\\).\n\n\n\n\n\n\nShape parameter\n\n\n\n\n\nScale parameter\n\n\n\n\n\n\n\n\n\n\nBe somewhat careful when using the gamma distribution in R. The gamma distribution is often parameterized using the rate parameter \\(\\beta=1/theta\\). If you are using the scale parameter, you need to name it explicitly and not rely on the position.\nIf the shape parameter is 1, then the gamma distribution is just the exponential distribution. It is extremely positively skewed. As the shape parameter increases, the gamma distribution becomes more and more symmetric, eventually converging to the normal distribution.\nThe chi-squared distribution is also a special case of the gamma distribution, with parameters \\(k=\\nu/2\\) and \\(\\theta=\\nu\\) (where \\(\\nu\\) is the degrees of freedom). Therefore, the gamma distribution is often used to model variances."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "",
    "text": "Florida State University\nThese are demonstrations which were written for EDF 5400, which is an introductory statistics class taught at the graduate level."
  },
  {
    "objectID": "index.html#index-of-r-demonstrations",
    "href": "index.html#index-of-r-demonstrations",
    "title": "Index of R Demonstrations for Intro Stat",
    "section": "Index of R Demonstrations",
    "text": "Index of R Demonstrations\n\n\n\n\nabout.qmd\n\n\nBinomialParms.qmd\n\n\nCentralLimitTheroem.qmd\n\n\nChi2calculator.qmd\n\n\nConditionalProbability.qmd\n\n\nConfidenceInterval.qmd\n\n\nCorrelation.qmd\n\n\nCorrelationExamples.qmd\n\n\nCorrelationExercise.qmd\n\n\nCorrelationOutliers.qmd\n\n\nCovidVaccines.qmd\n\n\nEffectSize.qmd\n\n\nGammaParams.qmd\n\n\nGeyser.qmd\n\n\nIndependence.qmd\n\n\nindex.qmd\n\n\nKurtosisBoxplots.qmd\n\n\nKurtosisPractice.qmd\n\n\nKurtosisQQ.qmd\n\n\nLawOfLargeNumbers.qmd\n\n\nLawOfLargeNumbersAnimated.qmd\n\n\nLogNormalParams.qmd\n\n\nNormalCalculator.qmd\n\n\nNormalParams.qmd\n\n\nPoissonParms.qmd\n\n\nRareDisease.qmd\n\n\nRegressionPrediction.qmd\n\n\nSkewnessBoxplot.qmd\n\n\nSkewnessPractice.qmd\n\n\nSkewnessQQ.qmd\n\n\nSlopeStandardErrors.qmd\n\n\nStandardDeviations.qmd\n\n\nStudenttcalculator.qmd\n\n\nTestCI.qmd\n\n\nVaccineCI.qmd\n\n\nZ-scores.qmd\n\n\n\n\n\n\n\nCC-BY\n\n\nThese are licensed under the creative commons CC BY license. You many distribute, remix, adapt, and build upon the material in any medium or format, so long as attribution is given to the creator.\nFor more information contact Russell Almond.\nThe Source files for these demonstrations can be found at https://pluto.coe.fsu.edu/svn/common/rgroup-shiny/IntroStats"
  },
  {
    "objectID": "NormalCalculator.html",
    "href": "NormalCalculator.html",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(z &lt; Z)\"=\"upper\",\n                          \"Lower tail: Pr(Z &lt; z)\"=\"lower\",\n                          \"Both tails: Pr(Z &lt;-z or z&lt; Z)\"=\"both\",\n                          \"Middle: Pr(-z &lt; Z &lt; z)\"=\"middle\"),\n              selected = \"both\"),\n  \n  numericInput(\"z\", label = \"Normal quantile (z):\", value=2)\n),\nmainPanel(\n  plotOutput(\"normcurve\")))\n\nserver &lt;- function (input,output) {\n  output$normcurve &lt;- \nrenderPlot({\n  q &lt;- input$z\n  p &lt;- switch(input$tails,\n              upper=1-pnorm(q),\n              lower=pnorm(q),\n              both=2*pnorm(-abs(q)),\n              middle=1-2*pnorm(-abs(q)))\n  xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n  curve(dnorm(x),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"z = \",round(q,3)),\n        xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"z\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n           polygon(cord.xu,cord.yu,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q)\n           cord.yl &lt;- c(0,dnorm(seq(-xl,q,0.01)),0)\n           polygon(cord.xl,cord.yl,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         both={\n           q &lt;- abs(q)\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n           polygon(cord.xu,cord.yu,col='skyblue')\n           cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q)\n           cord.yl &lt;- c(0,dnorm(seq(-xl,-q,0.01)),0)\n           polygon(cord.xl,cord.yl,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         },\n         middle={\n           q &lt;- abs(q)\n           cord.xmid &lt;- c(-q,seq(-q,q,0.01),q)\n           cord.ymid &lt;- c(0,dnorm(seq(-q,q,0.01)),0)\n           polygon(cord.xmid,cord.ymid,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         })\n  \n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "NormalCalculator.html#normal-probabilities.",
    "href": "NormalCalculator.html#normal-probabilities.",
    "title": "Normal Calculator",
    "section": "",
    "text": "In this tool, you input a z score, and get a corresponding normal probability.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  selectInput(\"tails\", label = \"Which tails\",\n              choices = c(\"Upper tail: Pr(z &lt; Z)\"=\"upper\",\n                          \"Lower tail: Pr(Z &lt; z)\"=\"lower\",\n                          \"Both tails: Pr(Z &lt;-z or z&lt; Z)\"=\"both\",\n                          \"Middle: Pr(-z &lt; Z &lt; z)\"=\"middle\"),\n              selected = \"both\"),\n  \n  numericInput(\"z\", label = \"Normal quantile (z):\", value=2)\n),\nmainPanel(\n  plotOutput(\"normcurve\")))\n\nserver &lt;- function (input,output) {\n  output$normcurve &lt;- \nrenderPlot({\n  q &lt;- input$z\n  p &lt;- switch(input$tails,\n              upper=1-pnorm(q),\n              lower=pnorm(q),\n              both=2*pnorm(-abs(q)),\n              middle=1-2*pnorm(-abs(q)))\n  xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n  curve(dnorm(x),main=paste(\"Probability of shaded region = \",round(p,3)),\n        sub=paste(\"z = \",round(q,3)),\n        xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"z\")\n  switch(input$tails,\n         upper={\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n           polygon(cord.xu,cord.yu,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         lower={\n           cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q)\n           cord.yl &lt;- c(0,dnorm(seq(-xl,q,0.01)),0)\n           polygon(cord.xl,cord.yl,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n         },\n         both={\n           q &lt;- abs(q)\n           cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n           cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n           polygon(cord.xu,cord.yu,col='skyblue')\n           cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q)\n           cord.yl &lt;- c(0,dnorm(seq(-xl,-q,0.01)),0)\n           polygon(cord.xl,cord.yl,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         },\n         middle={\n           q &lt;- abs(q)\n           cord.xmid &lt;- c(-q,seq(-q,q,0.01),q)\n           cord.ymid &lt;- c(0,dnorm(seq(-q,q,0.01)),0)\n           polygon(cord.xmid,cord.ymid,col='skyblue')\n           axis(1,q,paste(round(q,3)),cex.axis=2)\n           axis(1,-q,paste(-round(q,3)),cex.axis=2)\n         })\n  \n})\n}\nshinyApp(ui=ui,server=server)"
  },
  {
    "objectID": "NormalCalculator.html#normal-quantiles.",
    "href": "NormalCalculator.html#normal-quantiles.",
    "title": "Normal Calculator",
    "section": "Normal Quantiles.",
    "text": "Normal Quantiles.\nIn this tool, you input a probability, and get a corresponding z score.\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui1 &lt;- fluidPage(\n inputPanel(\n   selectInput(\"tails1\", label = \"Which tails\",\n               choices = c(\"Upper tail: Pr(z &lt; Z)\"=\"upper\",\n                           \"Lower tail: Pr(Z &lt; z)\"=\"lower\",\n                           \"Both tails: Pr(Z &lt;-z or z&lt; Z)\"=\"both\",\n                           \"Middle: Pr(-z &lt; Z &lt; z)\"=\"middle\"),\n               selected = \"both\"),\n   \n   numericInput(\"p\", label = \"Probability of shaded region:\", value=0.05, min=0, max=1)\n ),\n mainPanel(plotOutput(\"normcurve1\")))\n \nserver1 &lt;- function(input,output) {\n output$normcurve1 &lt;- renderPlot({\n   pp &lt;- input$p\n   q &lt;- switch(input$tails1,\n               upper=qnorm(1-pp),\n               lower=qnorm(pp),\n               both=qnorm(1-pp/2),\n               middle=qnorm(.5+pp/2))\n   xl &lt;- round(max(3,ceiling(abs(q)+.5)),1)\n   curve(dnorm(x),main=paste(\"Probability of shaded region = \",round(pp,3)),\n         sub=paste(\"z = \",round(q,3)),\n         xlim = c(-xl,xl),yaxt=\"n\",cex=3,cex.lab=2,cex.main=2,ylab=\"\",xlab=\"z\")\n   switch(input$tails1,\n          upper={\n            cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n            cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n            polygon(cord.xu,cord.yu,col='skyblue')\n            axis(1,q,paste(round(q,3)),cex.axis=2)\n          },\n          lower={\n            cord.xl &lt;- c(-xl,seq(-xl,q,0.01),q)\n            cord.yl &lt;- c(0,dnorm(seq(-xl,q,0.01)),0)\n            polygon(cord.xl,cord.yl,col='skyblue')\n            axis(1,q,paste(round(q,3)),cex.axis=2)\n          },\n          both={\n            q &lt;- abs(q)\n            cord.xu &lt;- c(q,seq(q,xl,0.01),xl)\n            cord.yu &lt;- c(0,dnorm(seq(q,xl,0.01)),0)\n            polygon(cord.xu,cord.yu,col='skyblue')\n            cord.xl &lt;- c(-xl,seq(-xl,-q,0.01),-q)\n            cord.yl &lt;- c(0,dnorm(seq(-xl,-q,0.01)),0)\n            polygon(cord.xl,cord.yl,col='skyblue')\n            axis(1,q,paste(round(q,3)),cex.axis=2)\n            axis(1,-q,paste(-round(q,3)),cex.axis=2)\n          },\n          middle={\n            q &lt;- abs(q)\n            cord.xmid &lt;- c(-q,seq(-q,q,0.01),q)\n            cord.ymid &lt;- c(0,dnorm(seq(-q,q,0.01)),0)\n            polygon(cord.xmid,cord.ymid,col='skyblue')\n            axis(1,q,paste(round(q,3)),cex.axis=2)\n            axis(1,-q,paste(-round(q,3)),cex.axis=2)\n          })\n   \n })\n}\nshinyApp(ui1, server1)"
  },
  {
    "objectID": "BinomialParms.html",
    "href": "BinomialParms.html",
    "title": "Binomial Parameters",
    "section": "",
    "text": "The binomial distribution can be thought of as a number of draws, \\(n\\), from an urn with a proportion \\(p\\), of black balls.\nThe probability of drawing exactly \\(x\\) balls from an this urn is: \\[ p(X|n,p) = \\binom{n}{X} p^X (1-p)^{n-X}\\]\nThe expected value is \\(np\\), and the standard deviation is \\(\\sqrt{np(1-p)}\\).\nSometimes we write this in terms of the proportion of black balls in the sample. That is \\(p\\), with a standard deviation of \\(\\sqrt{p(1-p)/n}\\).\n#| standalone: true\n#| viewerHeight: 500\nlibrary(shiny)\nui &lt;- fluidPage(\ninputPanel(\n  sliderInput(\"n\", label = \"Number of draws:\",\n              min=0, max=100, value=10, step=1),\n  \n  sliderInput(\"p\", label = \"Probability of success:\",\n              min = 0, max = 1, value = .6, step = 0.01)\n),\nmainPanel(\n  plotOutput(\"bincurve\")))\n\nserver &lt;- function (input,output) {\n  output$bincurve &lt;- renderPlot({\n  n &lt;- as.numeric(input$n)\n  p &lt;- as.numeric(input$p)\n  dat &lt;- data.frame(x=0:n,y=dbinom(0:n,n,p))\n  ggplot(dat,aes(x,y)) +geom_col()  \n\n})\n}\nshinyApp(ui=ui,server=server)\nNote that this distribution is positively skewed if \\(p &lt; 0.5\\) and negatively skewed if \\(p &gt; 0.5\\).\nNote how when \\(n\\) gets large, the binomial distribution looks a lot like the normal. This is one of the first central limit theorems that was discovered. (The closer that \\(p\\) is to 0 or 1, the longer convergence to the normal takes.)"
  },
  {
    "objectID": "SkewnessBoxplot.html",
    "href": "SkewnessBoxplot.html",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "SkewnessBoxplot.html#skewness-determination-exercise.",
    "href": "SkewnessBoxplot.html#skewness-determination-exercise.",
    "title": "Skewness Boxplot Practice",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: A, B and C. These will be randomly assigned to a positively skewed, negatively skewed, and symmetric distribution type. Your job is to determine which is which.\nYou can redraw from the same distributions by changing the sample size.\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the skewness of each distribution.\n\n\n\n\n\n\nA\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nB\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\nC\n\nUnknown\nNegatively Skewed\nPositively Skewed\nSymmetric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly."
  },
  {
    "objectID": "SkewnessBoxplot.html#what-to-look-for",
    "href": "SkewnessBoxplot.html#what-to-look-for",
    "title": "Skewness Boxplot Practice",
    "section": "What to look for:",
    "text": "What to look for:\n\nIs the box from median to quartile longer on one side than the other?\nIs the whisker longer on one side than the other?\nAre there outliers on one side and not the other?\n\nAll three of these are signs of skewness in that direction (longer box, whisker, or outliers)."
  },
  {
    "objectID": "SkewnessBoxplot.html#related-pages",
    "href": "SkewnessBoxplot.html#related-pages",
    "title": "Skewness Boxplot Practice",
    "section": "Related Pages",
    "text": "Related Pages\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "RareDisease.html",
    "href": "RareDisease.html",
    "title": "Rare Disease – COVID-19",
    "section": "",
    "text": "The rare disease problem is one of those “paradoxes” of statistics. The results are surprising because their are two sources of information: the prevalence of the disease in the population and the accuracy of the test. Often the former is stronger evidence than the latter, so people find it surprising.\nThe question of interest is: “What is the probability that a patient has the disease, given that the patient tests positive?” (This is sometimes called the True Positive probability; one minus would be called the False Positive probability.) A related question of interest is “What is the probability that a patient does not have the disease given that the patient tests negative?” (This is the False Negative probability.)\nStart by defining some variables. Let \\(D \\in \\{Y,N\\}\\) be whether or not a given individual has the disease, and let \\(T\\in\\{+,-\\}\\) be whether that individual gets a positive or negative result on the test. The joint probability of \\(D, T\\) is characterized by three numbers:\nLets try an example. A company called BioResponse just (March 19, 2020) launched the CoronaCheck test kit (Press Release). This article reports: “Our manufacturers report a sensitivity of 97.2% and specificity of 92%.”\nNow the hard part: estimating the base rate. This is hard because (a) people can have very mild symptoms for days and not know they have the disease, and (b) there has been a general shortage of test kits. So relying on official numbers is likely to give a big underestimate. As I’m not planning on updating this web site in real time, the numbers I’m putting in here will be out of date by the time you read this.\nAs I live in Florida, I’ll use the official Florida numbers: https://floridahealthcovid19.gov/#latest-stats . As of 2020-03-22 18:000, there were 1007 known cases in Florida, which has a population of 21,992,985. That gives a base rate of 4.5787327^{-5}. For the US, the number is 33,276 known cases, and a population of 330,464,151 for a base rate of 1.0069473^{-4}.\nUpdate: As of 2020-09-02, the state of Florida is reporting 624,116 cases, for a base rate of 0.028378. Note that the number of known cases is smaller than the total number of cases (especially, as we have learned that some people get very mild symptoms and may not know they are sick to seek testing).\n\\(Update^2\\): I have found a web site which gives background rates for SARS-COV-2 by state and county, so you can get local information. Hopefully, they are updating with the latest numbers. microCOVID Project.\nBase Rate (Pr(D=Y)):\n\n\n\n\n\nSensitivity (Pr(T=+|D=Y)):\n\n\n\n\n\nSpecificity (Pr(T=-|D=N)):"
  },
  {
    "objectID": "RareDisease.html#calculating-true-positive-and-false-positive.",
    "href": "RareDisease.html#calculating-true-positive-and-false-positive.",
    "title": "Rare Disease – COVID-19",
    "section": "Calculating true positive and false positive.",
    "text": "Calculating true positive and false positive.\nOne way to calculate this is to use Bayes’ theorem. However, from the table above, it is easy to calculate the true positive and false positive rates. We now just look at the columns of the table.\n\n\n\nFalse Positive Rate\nPr(D=N|T=+)\n\n\n\n\n\nFalse Negative Rate\nPr(D=Y|T=-)\n\n\n\n\nWhat is going on???\nThat false positive rate seems very high. What is really going on? The root cause is that as of this writing (Mar 22, 2020) COVID-19 is still pretty rare. So although getting a false positive is rare, actually having COVID-19 is much rarer. The following picture might help:\n\n\n\n\n\nThe thin bar on the left represents people who have COVID-19. There are still (fortunately) very few of them. The bar on top represents the false positives, fortunately, there are still a lot more of them than the true positives, so true positives are still rare. (May it always be so).\nOn the other hand, the false negative rate is very comforting. It means that if you test negative, you can be pretty sure that it is safe for you to be around other people (especially the old or sick)."
  },
  {
    "objectID": "RareDisease.html#sensitivity-analysis",
    "href": "RareDisease.html#sensitivity-analysis",
    "title": "Rare Disease – COVID-19",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\nDon’t forget that these base rates are underestimates. There is currently a shortage of tests, so these are only cases that have actually be able to be tested. Also, symptoms can take up to 3 days to appear, so some people who have it, probably don’t even know that they should ask to be tested. The actual infection rate could be 10 or more times as high as the known infection rate.\nAlso, there are various risk factors which should be added to the base rate. If the person being tested has traveled lately to an area with a higher rate, the base rate should go up. So too if the person has a fever or other symptoms of the virus.\nSo, play around with the base rate. Play with the sensitivity and specificity? How does this change? This will help you get a better feel for how the rare disease problem works.\nFinally, don’t forget that this thing grows exponentially fast (that is why it is a pandemic). This number could be go up very quickly. Here is an explanation.. ( As an aside, this is the kind of thing we would analyze on the log scale.)"
  },
  {
    "objectID": "RareDisease.html#how-would-this-test-be-used.",
    "href": "RareDisease.html#how-would-this-test-be-used.",
    "title": "Rare Disease – COVID-19",
    "section": "How would this test be used.",
    "text": "How would this test be used.\nActually, the most interesting thing about the CoronaCheck kit is that it only takes 15 minutes. This is great considering the older test takes 3 days. So assuming BioResponse can produce these quickly (or that other vendors come online with similar tests), these can be used for screening (say health care workers, or other first responders), as well as people presenting with other symptoms or having recently traveled.\nIf these people test positive on the quick screening test, they should be isolated and possibly a more sensitive (and probably time consuming) test be given. If they test negative, then they can be cleared to go about their normal activity. I’m sure this is how this test will be used.\nAnother factor is that doctors are simply not giving out tests unless there are other risk factors. I was in my doctors office for my daughter’s physical and talking to the nurse. She said that there was a woman who was tired (needs more sleep?) and congested (this is Tallahassee in March, the trees are raining pollen), but no fever. The nurse had to explain that she didn’t have enough test to give out unless there were more symptoms (particularly a fever). This will change as our testing capacity gets better (last I looked, Mar 20, the US was still doing only about 1/2 the number of test per capita as South Korea."
  },
  {
    "objectID": "RareDisease.html#dont-break-lockdownself-isolation",
    "href": "RareDisease.html#dont-break-lockdownself-isolation",
    "title": "Rare Disease – COVID-19",
    "section": "Don’t break lockdown/self-isolation",
    "text": "Don’t break lockdown/self-isolation\nDon’t panic, but do not be complacent either.\nSome of you reading this will be in official lockdown. Others will be under a self-distancing protocol. This is still extremely important as (1) the base rate will rise over time, probably quite quickly and (2) the disease takes up to 3 days to get started and the symptoms might appear like a common cold (Novel Coronavirus 19 is in fact an uncommon cold). You might have it and not know it yet. If you break the self-distancing protocol, you could be another Typhoid Mary spreading sickness and misery all around you.\nOh, and congrats to BioResponse on their breakthrough. I can’t judge the quality of the numbers from just a press release, but if they really can make that number of tests, that would be a big help. I hope lots of other biotech companies are working on this problem, too.\nStay healthy. Keep your distance. Wash your hands, and obey the local health authorities. Lets make sure we keep that base rate (i.e., the infection rate) low."
  },
  {
    "objectID": "Correlation.html",
    "href": "Correlation.html",
    "title": "Correlation Coefficient",
    "section": "",
    "text": "This demonstration will use some random data. Lets start by generating the random data. So give a random seed and pick a sample size for your sample.\nSample Size:\n\n25\n50\n100\n250\n500\n1000\n\n\n\n\n\n\nRandom number Seed (integer)"
  },
  {
    "objectID": "Correlation.html#regression.",
    "href": "Correlation.html#regression.",
    "title": "Correlation Coefficient",
    "section": "Regression.",
    "text": "Regression.\nGalton’s discovery was that if you want to predict \\(Y\\) from \\(X\\), then you want to regress that prediction towards the mean. If \\(X\\) and \\(Y\\) were perfectly correlated, then the \\(z\\)-score for a variable on the \\(X\\) scale would be the same for the variable on the \\(Y\\) scale, so all we need to do is change units. The ratio \\(\\sigma_Y/\\sigma_X\\) changes units from \\(X\\) to \\(Y\\). We also want the mean of \\(X\\) to map to the mean of \\(Y\\). The equation for this change-of-units line, the SD-line, is: \\[ \\widetilde y = \\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] The first term is the change of units, the second term makes sure the line goes through the mean of \\(X\\) and the mean of \\(Y\\).\nThe ideal discount is the correlation coefficient \\(\\rho_{XY}\\). This gives the the following final regression line: \\[\\widehat y = \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} x + \\left ( \\mu_Y - \\rho_{XY}\\frac{\\sigma_Y}{\\sigma_X} \\mu_X\\right ) .\\] Because second term has \\(\\rho_{XY}\\) in it as well, this will make the predicted value closer to the mean of \\(Y\\), \\(\\mu_Y\\).\n The notations \\(\\widetilde{y}\\) and \\(\\widehat{y}\\) indicate predicted values for \\(y\\). The y hat (\\(\\widehat y\\)) notation is reserved for what is called maximum likelihood predictions. In the case of a regression with normally distributed errors, the maximum likelihood predictor is also the least squares estimator. Usually, the estimators use the sample values, thus in the regression equation: \\[ \\widehat{b_1} = r_{XY}\\frac{s_X}{s_Y}\\;;\\qquad \\widehat{b_0}=\\bar Y -r_{XY}\\frac{s_X}{s_Y} \\bar X\\; .\\]"
  },
  {
    "objectID": "Correlation.html#lets-try-it.",
    "href": "Correlation.html#lets-try-it.",
    "title": "Correlation Coefficient",
    "section": "Lets Try it.",
    "text": "Lets Try it.\nIn the graph below, you can set the mean and standard deviation of both X and Y as well as the correlation coefficient. The SD line is a dashed blue, and the regression line is a solid red. Play around for a bit.\n\n\n\n\n\n\nMean of X:\n\n\n\n\n\nStandard Deviation of X:\n\n\n\n\n\nMean of Y:\n\n\n\n\n\nStandard Deviation of Y:\n\n\n\n\n\nCorrelation between X and Y:\n\n\n\n\n\n\n\n\n\n\nNotice how changing the means and standard deviations doesn’t change much except the numbers in the equations and the labels on the axis. This is because R is automatically adjusting the scale of the graphs to fit the data. In this view, the SD line, the one that is a change of scale, is a perfect 45 degress, no matter what.\nActually, if you set the correlation coefficient to be the same in the two plots, they should look the same. The two plots show the same data, its just in the lower plot, the data are transformed to fit the statistis. Correlation is a property that is independent from Mean and SD."
  },
  {
    "objectID": "Correlation.html#for-the-more-mathematically-inclined.",
    "href": "Correlation.html#for-the-more-mathematically-inclined.",
    "title": "Correlation Coefficient",
    "section": "For the more mathematically inclined.",
    "text": "For the more mathematically inclined.\nThis is how I generated the data with the given correlations.\nFor the first plot, I first generated two vectors of standard normal numbers, \\({\\bf X}\\) and \\({\\bf e}\\), that is with mean 0 and standard deviation 1. Then I defined \\({\\bf Y}\\) with the following equation: \\[ Y_i = \\rho_{XY}X_i + \\sqrt{(1-\\rho_{XY}^2)}\\ e_i\\] As \\(\\rho_{XY}^2 + (1-\\rho_{XY}^2) =1\\), \\({\\bf Y}\\) also has mean 0 and standard deviation 1.\nFor the second plot, I used the following expressions: \\[ XX_i = \\mu_X + \\sigma_X X_i\\;; \\qquad YY_i = \\mu_X + \\sigma_Y Y_i\\;.\\] The transformation is exactly the same."
  },
  {
    "objectID": "ConditionalProbability.html",
    "href": "ConditionalProbability.html",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "ConditionalProbability.html#some-example-data.",
    "href": "ConditionalProbability.html#some-example-data.",
    "title": "ConditionalProbability",
    "section": "",
    "text": "On Oct 1, Merck announced exciting results for a new drug, Molnupiravir a pill for treating patients with COVID-19. The results were good enough by 29 days into the study, that they submitted the results to the FDA for emergency approval. (Note that the link above is to a press release and not to a scientific paper that has been peer or FDA review; but if the results hold up to scrutiny, this could be exciting.)\nHere is the relevant bit of the press release: &gt; At the interim analysis, molnupiravir reduced the risk of hospitalization or death by approximately 50%; 7.3% of patients who received molnupiravir were either hospitalized or died through Day 29 following randomization (28/385), compared with 14.1% of placebo-treated patients (53/377); p=0.0012.\nFrom this we can construct the following data table.\n\nn &lt;- c(drug=385,placebo=377)\nhd &lt;- c(drug=28,placebo=53)\nnhd &lt;- n-hd\ntab &lt;- data.frame(hd,nhd,n)\ntab1 &lt;- rbind(tab,Total=colSums(tab))\ntab1\n\n        hd nhd   n\ndrug    28 357 385\nplacebo 53 324 377\nTotal   81 681 762\n\n\nA mosaic plot allows us to look at this table graphically.\n\n## Subset to just the inner part of the table.\nppddat &lt;- as.matrix(tab[,1:2])\nnames(dimnames(ppddat)) &lt;- c(\"Treatment\",\"Outcome\")\nstrucplot(ppddat,labeling=labeling_values(\"observed\"))"
  },
  {
    "objectID": "ConditionalProbability.html#pr-ohd",
    "href": "ConditionalProbability.html#pr-ohd",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD)",
    "text": "Pr (O=HD)\nWith no bar, indicating no conditioning, we are looking at the probability among all people in the sample. This is called the marginal probability because it comes from the margins (sums) of the table.\n\n\n\n\n\n\n\n\n\nThe marginal (no condition) probability of a negative (hospitalization or death) outcome is 81 (red areas)/762 (blue areas) = 0.106."
  },
  {
    "objectID": "ConditionalProbability.html#pr-ohd-t-drug",
    "href": "ConditionalProbability.html#pr-ohd-t-drug",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Drug)",
    "text": "Pr (O=HD | T = Drug)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the active treatment.\n\n\n\n\n\n\n\n\n\nThe condtional probability of a negative (hospitalization or death) outcome given the drug is 28 (red areas)/385 (purple areas) = 0.073."
  },
  {
    "objectID": "ConditionalProbability.html#pr-ohd-t-placebo",
    "href": "ConditionalProbability.html#pr-ohd-t-placebo",
    "title": "ConditionalProbability",
    "section": "Pr (O=HD | T = Placebo)",
    "text": "Pr (O=HD | T = Placebo)\nThe bar conditions or restricts the sample to just the people who meet the condition, in this case, those that have taken the placebo treatment.\n\n\n\n\n\n\n\n\n\nTThe condtional probability of a negative (hospitalization or death) outcome given the placebo is 53 (red areas)/377 (purple areas) = 0.141."
  },
  {
    "objectID": "ConditionalProbability.html#independence",
    "href": "ConditionalProbability.html#independence",
    "title": "ConditionalProbability",
    "section": "Independence",
    "text": "Independence\nIf the drug was independent of hospitalization, then the two probabilities we calculated above should be the same. In fact, when we calculate the ratio, we get 0.517. A huge improvement. No wonder Merck was so excited.\nWe should also be able to look at the column probabilities in the same way. The original sample is close to a 50-50 split between drug and placebo. (It actually 0.505, probably because they were not finished their recruting. This little bit of unbalance doesn’t matter). However, when we look at the fraction of the people who were hospitalized who got the drug \\(\\Pr(Treatment=drug | Outcome=HD)\\) we see it is 0.346. Once again this is very different.\nSuppose we have two variables \\(A\\) with possible outcomes \\(\\{a_1,\\ldots,a_J\\}\\) and \\(B\\) with possible outcomes \\(\\{b_1,\\ldots,b_K\\}\\). If \\(A\\) and \\(B\\) are independent, three different relationships will hold:\n\n\\(\\Pr(A=a_j | B=b_k) = \\Pr(A=a_j | B=b_k') = \\Pr(A=a_j) \\qquad \\forall j,k,k'\\)\n\\(\\Pr(B=b_k | A=a_j) = \\Pr(B=b_k | A=a_j') = \\Pr(B=b_j) \\qquad \\forall j,j',k\\)\n\\(\\Pr(A=a_j \\wedge B=b_k) = \\Pr(A=a_j) \\Pr(B=b_k) \\qquad \\forall j,k\\) (where \\(\\wedge\\) means and).\n\nRecall that the conditional probability is formally defined as \\[ \\Pr(A=a_j|B=b_k) = \\frac{\\Pr(A=a_j \\wedge B=b_k)}{\\Pr(B=b_k)} \\ .\\] So if \\(\\Pr(B=b_k)=0\\) for any \\(k\\), then the first expression doesn’t quite work right because we need to divide by zero. Similarly, the second has problems if \\(\\Pr(A=a_j)=0\\) for any \\(j\\). Therfore, the thrid expression is used as the definition, because it avoids the technical problems (divide by zero in certain cases) of the first two. But I think the first two give better intuition for what independence means."
  },
  {
    "objectID": "ConditionalProbability.html#the-chi-square-test",
    "href": "ConditionalProbability.html#the-chi-square-test",
    "title": "ConditionalProbability",
    "section": "The Chi-square test",
    "text": "The Chi-square test\nNote that even though the experimental design called for equal numbers in the drug and placebo arms of the studies, random events in how the participants were recruited made them slightly unequal. This is likely just luck, and not a serious problem.\nNow look at the different probabilities of a negative outcome for the drug and the placebo. How do we know if that is real, or just luck? One way we could answer that question is to build a statistical model for “just luck” and calculate how like the the observed data are if that model is true.\nIn this case, the model for “just luck” is the independence value up above. Using that we can calculate the expected values."
  },
  {
    "objectID": "KurtosisQQ.html",
    "href": "KurtosisQQ.html",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "KurtosisQQ.html#kurtosis-determination-exercise.",
    "href": "KurtosisQQ.html#kurtosis-determination-exercise.",
    "title": "Kurtosis Practice using Quantile-Quantile Plots",
    "section": "",
    "text": "In this exercise, the computer will generate 3 datasets: X, Y and Z. These will be randomly assigned to high (leptokurtic), medium (mesokurtic) and low (platykurtic) distributions. The data (sorted in order) are plotted on the Y axis and the quantiles of a standard normal (qnorm) distribution are plotted on the X axis. A normal distribution should appear as a straight line; leptokurtic and platykurtic distributions as ‘S’ or ‘Z’ curves. (A ‘U’ or ‘C’ shaped curve indicates skewness, not kurtosis.) Note: SPSS plots the normal quantiles on the Y axis and the data on the X: Which means that leptokurtic and platykurtic distributions will curve in the opposite direction from these Q-Q plots.\nYou can redraw from the same distributions by changing the sample size (higher sample sizes are easier to see).\n\n\n\n\n\n\nSample Size:\n\n50\n100\n500\n1000\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify the kurtosis of each distribution.\n\n\n\n\n\n\nX\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nY\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\nZ\n\nUnknown\nPlatykurtic (flat)\nLeptokurtic (heavy tails)\nMesokurtic (normal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo try again with different distributions, reload the page. If you are having trouble, try increasing the sample size: sometimes a small sample won’t display the characteristics of the distribution strongly.\nHere are the other exercises in this series:\n\nSkewness Practice:\n\nHistograms\nBoxplots\nQ-Q Plots\n\nKurtosis Practice:\n\nHistograms\nBoxplots\nQ-Q Plots"
  },
  {
    "objectID": "StandardDeviations.html",
    "href": "StandardDeviations.html",
    "title": "Standard Deviations",
    "section": "",
    "text": "This R Markdown document is made interactive using Shiny. Unlike the more traditional workflow of creating static reports, you can now create documents that allow your readers to change the assumptions underlying your analysis and see the results immediately.\nTo learn more, see Interactive Documents."
  },
  {
    "objectID": "StandardDeviations.html#inputs-and-outputs",
    "href": "StandardDeviations.html#inputs-and-outputs",
    "title": "Standard Deviations",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\nYou can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change. This demonstrates how a standard R plot can be made interactive by wrapping it in the Shiny renderPlot function. The selectInput and sliderInput functions create the input widgets used to drive the plot.\n\nlibrary(shiny)\ndata &lt;- rnorm(15,mean=0,sd=2)\nplot(data,1:length(data))\nabline(v=0)\nsegments(mean(data),1:length(data),data,1:length(data))\n\n\n\n\n\n\n\nsd(data)\n\n[1] 1.584272\n\nhist(data)\n\n\n\n\n\n\n\n\n\\[ \\sqrt(\\sum (X_i - \\mu)^2/N) \\]\n\ndata1 &lt;- c(11,13,14,15,17,18,19,20)\ndata2 &lt;- c(12,15,15,15,15,15,15,15)\ndata3 &lt;- c(5,13,19,24,33,38,51,70)\ndata4 &lt;- c(11,12,13,16,18,20,22,23)\npar(mfrow=c(2,2))\nplot(data1,1:length(data1),main=\"Data 1\",xlim=c(0,50))\nabline(v=mean(data1))\nsegments(mean(data1),1:length(data1),data1,1:length(data1))\nsd(data1)\n\n[1] 3.136764\n\nplot(data2,1:length(data2),main=\"Data 2\",xlim=c(0,50))\nabline(v=mean(data2))\nsegments(mean(data2),1:length(data2),data2,1:length(data2))\nsd(data2)\n\n[1] 1.06066\n\nplot(data3,1:length(data3),main=\"Data 3\",xlim=c(0,50))\nabline(v=mean(data3))\nsegments(mean(data3),1:length(data3),data3,1:length(data3))\nsd(data3)\n\n[1] 21.25987\n\nplot(data4,1:length(data4),main=\"Data 4\",xlim=c(0,50))\nabline(v=mean(data4))\nsegments(mean(data4),1:length(data4),data4,1:length(data4))\n\n\n\n\n\n\n\nsd(data4)\n\n[1] 4.611709\n\n\n\n\n\n\n\n\nNumber of bins:\n\n10\n20\n35\n50\n\n\n\n\n\n\nBandwidth adjustment:"
  },
  {
    "objectID": "StandardDeviations.html#embedded-application",
    "href": "StandardDeviations.html#embedded-application",
    "title": "Standard Deviations",
    "section": "Embedded Application",
    "text": "Embedded Application\nIt’s also possible to embed an entire Shiny application within an R Markdown document using the shinyAppDir function. This example embeds a Shiny application located in another directory:\n\n\n\n\n\nNote the use of the height parameter to determine how much vertical space the embedded application should occupy.\nYou can also use the shinyApp function to define an application inline rather then in an external directory.\nIn all of R code chunks above the echo = FALSE attribute is used. This is to prevent the R code within the chunk from rendering in the document alongside the Shiny components."
  },
  {
    "objectID": "LogNormalParams.html",
    "href": "LogNormalParams.html",
    "title": "Normal Parameters",
    "section": "",
    "text": "A parameter is a value that can be changed in a statistical model. For example, the mean and standard deviation are the parameters of the normal distribution, which is a model for a population. Changing the value of a parameter, changes the model. We can see that in the illustration below. Try changing the values of the mean and standard deviation and see what happens to the shape of the curve."
  },
  {
    "objectID": "LogNormalParams.html#inputs-and-outputs",
    "href": "LogNormalParams.html#inputs-and-outputs",
    "title": "Normal Parameters",
    "section": "Inputs and Outputs",
    "text": "Inputs and Outputs\n\n\n\n\n\n\nMean Log:\n\n\n\n\n\nStandard Deviation Log:"
  },
  {
    "objectID": "LogNormalParams.html#scale-and-location-parameters",
    "href": "LogNormalParams.html#scale-and-location-parameters",
    "title": "Normal Parameters",
    "section": "Scale and Location Parameters",
    "text": "Scale and Location Parameters\nThe mean has a special role in the normal distribution; it determines where the center of the curve is. This makes it a location parameter.\nThe standard deviation has a special role in the normal distribution; it streches and shrinks the curve around the mean. This makes it a scale parameter.\nSometimes, the effects of scale and location parameters can be hard to see. This is because most statistical graphcis packages adjust the axis of the graph, so that the curve will always appear centered in the plotting window. In the normal curve above, I fixed the plotting window so that you can see the curve move. In the example below, I let the plotting window adjust with the curve. Notice how the curve stays the same, but the labels on the axis change.\n\n\n\n\n\n\nMean:\n\n\n\n\n\nStandard Deviation:"
  }
]